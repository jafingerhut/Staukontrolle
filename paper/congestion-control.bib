

# Andy file name:
# 1994-brakmo-et-al-tcp-vegas-new-techniques-for-congestion-detection-and-avoidance.pdf
@inproceedings{BOP1994,
author = {Brakmo, Lawrence S. and O'Malley, Sean W. and Peterson, Larry L.},
title = {TCP Vegas: new techniques for congestion detection and avoidance},
year = {1994},
isbn = {0897916824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/190314.190317},
doi = {10.1145/190314.190317},
abstract = {Vegas is a new implementation of TCP that achieves between 40 and 70\% better throughput, with one-fifth to one-half the losses, as compared to the implementation of TCP in the Reno distribution of BSD Unix. This paper motivates and describes the three key techniques employed by Vegas, and presents the results of a comprehensive experimental performance study—using both simulations and measurements on the Internet—of the Vegas and Reno implementations of TCP.},
booktitle = {Proceedings of the Conference on Communications Architectures, Protocols and Applications},
pages = {24–35},
numpages = {12},
location = {London, United Kingdom},
series = {SIGCOMM '94}
}

# alternate url:
# https://cseweb.ucsd.edu/classes/wi01/cse222/papers/brakmo-vegas-jsac95.pdf
# Andy file name:
# 1995-brakmo-et-al-tcp-vegas-end-to-end-congestion-avoidance-on-a-global-internet.pdf
@article{BP1995,
  author={Brakmo, L.S. and Peterson, L.L.},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={TCP Vegas: end to end congestion avoidance on a global Internet}, 
  year={1995},
  volume={13},
  number={8},
  pages={1465-1480},
  keywords={Internet;Protocols;Throughput;Testing;Bandwidth;Programmable control;Adaptive control;Jacobian matrices;Computer science;TCPIP},
  doi={10.1109/49.464716}
}

# Andy file name:
# 1996-hoe-improving-the-start-up-behavior-of-a-congestion-control-scheme-for-tcp.pdf
@inproceedings{Hoe1996,
author = {Hoe, Janey C.},
title = {Improving the start-up behavior of a congestion control scheme for TCP},
year = {1996},
isbn = {0897917901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/248156.248180},
doi = {10.1145/248156.248180},
abstract = {Based on experiments conducted in a network simulator and over real networks, this paper proposes changes to the congestion control scheme in current TCP implementations to improve its behavior during the start-up period of a TCP connection.The scheme, which includes Slow-start, Fast Retransmit, and Fast Recovery algorithms, uses acknowledgments from a receiver to dynamically calculate reasonable operating values for a sender's TCP parameters governing when and how much a sender can pump into the network. During the start-up period, because a TCP sender starts with default parameters, it often ends up sending too many packets and too fast, leading to multiple losses of packets from the same window. This paper shows that recovery from losses during this start-up period is often unnecessarily time-consuming.In particular, using the current Fast Retransmit algorithm, when multiple packets in the same window are lost, only one of the packet losses may be recovered by each Fast Retransmit; the rest are often recovered by Slow-start after a usually lengthy retransmission timeout. Thus, this paper proposes changes to the Fast Retransmit algorithm so that it can quickly recover from multiple packet losses without waiting unnecessarily for the timeout. These changes, tested in the simulator and on the real networks, show significant performance improvements, especially for short TCP transfers. The paper also proposes other changes to help minimize the number of packets lost during the start-up period.},
booktitle = {Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications},
pages = {270–280},
numpages = {11},
location = {Palo Alto, California, USA},
series = {SIGCOMM '96}
}

# alternate url:
# https://www.cs.princeton.edu/courses/archive/fall16/cos561/papers/Cubic08.pdf
# Andy file name:
# 2008-ha-et-al-cubic-a-new-tcp-friendly-high-speed-tcp-variant.pdf
@article{HRX2008,
author = {Ha, Sangtae and Rhee, Injong and Xu, Lisong},
title = {CUBIC: a new TCP-friendly high-speed TCP variant},
year = {2008},
issue_date = {July 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/1400097.1400105},
doi = {10.1145/1400097.1400105},
abstract = {CUBIC is a congestion control protocol for TCP (transmission control protocol) and the current default TCP algorithm in Linux. The protocol modifies the linear window growth function of existing TCP standards to be a cubic function in order to improve the scalability of TCP over fast and long distance networks. It also achieves more equitable bandwidth allocations among flows with different RTTs (round trip times) by making the window growth to be independent of RTT -- thus those flows grow their congestion window at the same rate. During steady state, CUBIC increases the window size aggressively when the window is far from the saturation point, and the slowly when it is close to the saturation point. This feature allows CUBIC to be very scalable when the bandwidth and delay product of the network is large, and at the same time, be highly stable and also fair to standard TCP flows. The implementation of CUBIC in Linux has gone through several upgrades. This paper documents its design, implementation, performance and evolution as the default TCP algorithm of Linux.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jul},
pages = {64–74},
numpages = {11}
}

# Andy file name:
# 2010-alizadeh-et-al-data-center-tcp-dctcp.pdf
@inproceedings{AGMPe2010,
author = {Alizadeh, Mohammad and Greenberg, Albert and Maltz, David A. and Padhye, Jitendra and Patel, Parveen and Prabhakar, Balaji and Sengupta, Sudipta and Sridharan, Murari},
title = {Data center TCP (DCTCP)},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851192},
doi = {10.1145/1851182.1851192},
abstract = {Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry "background" flows build up queues at the switches, and thus impact the performance of latency sensitive "foreground" traffic.To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90\% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {63–74},
numpages = {12},
keywords = {data center network, TCP, ECN},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

# alternate url:
# https://conferences.sigcomm.org/sigcomm/2011/papers/sigcomm/p50.pdf
# Andy file name:
# 2011-wilson-et-al-better-never-than-late-meeting-deadlines-in-datacenter-networks.pdf
@inproceedings{WBKR2011,
author = {Wilson, Christo and Ballani, Hitesh and Karagiannis, Thomas and Rowtron, Ant},
title = {Better never than late: meeting deadlines in datacenter networks},
year = {2011},
isbn = {9781450307970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2018436.2018443},
doi = {10.1145/2018436.2018443},
abstract = {The soft real-time nature of large scale web applications in today's datacenters, combined with their distributed workflow, leads to deadlines being associated with the datacenter application traffic. A network flow is useful, and contributes to application throughput and operator revenue if, and only if, it completes within its deadline. Today's transport pro- tocols (TCP included), given their Internet origins, are agnostic to such flow deadlines. Instead, they strive to share network resources fairly. We show that this can hurt application performance.Motivated by these observations, and other (previously known) deficiencies of TCP in the datacenter environment, this paper presents the design and implementation of D3, a deadline-aware control protocol that is customized for the datacenter environment. D3 uses explicit rate control to apportion bandwidth according to flow deadlines. Evaluation from a 19-node, two-tier datacenter testbed shows that D3, even without any deadline information, easily outper- forms TCP in terms of short flow latency and burst tolerance. Further, by utilizing deadline information, D3 effectively doubles the peak load that the datacenter network cansupport.},
booktitle = {Proceedings of the ACM SIGCOMM 2011 Conference},
pages = {50–61},
numpages = {12},
keywords = {sla, rate control, online services, deadline, datacenter},
location = {Toronto, Ontario, Canada},
series = {SIGCOMM '11}
}

# alternate url:
# https://www.usenix.org/legacy/events/nsdi11/tech/full_papers/Wischik.pdf
# Andy file name:
# todo
@inproceedings{WRGH2011,
author = {Wischik, Damon and Raiciu, Costin and Greenhalgh, Adam and Handley, Mark},
title = {Design, implementation and evaluation of congestion control for multipath TCP},
year = {2011},
publisher = {USENIX Association},
address = {USA},
abstract = {Multipath TCP, as proposed by the IETF working group mptcp, allows a single data stream to be split across multiple paths. This has obvious benefits for reliability, and it can also lead to more efficient use of networked resources. We describe the design of a multipath congestion control algorithm, we implement it in Linux, and we evaluate it for multihomed servers, data centers and mobile clients. We show that some 'obvious' solutions for multipath congestion control can be harmful, but that our algorithm improves throughput and fairness compared to single-path TCP. Our algorithmis a drop-in replacement for TCP, and we believe it is safe to deploy.},
booktitle = {Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation},
pages = {99–112},
numpages = {14},
location = {Boston, MA},
series = {NSDI'11}
}

# alternate url:
# https://web.stanford.edu/~balaji/papers/11analysisof.pdf
# Andy file name:
# 2011-alizadeh-et-al-analysis-of-dctcp-stability-convergence-and-fairness.pdf
@inproceedings{AJP2011,
author = {Alizadeh, Mohammad and Javanmard, Adel and Prabhakar, Balaji},
title = {Analysis of DCTCP: stability, convergence, and fairness},
year = {2011},
isbn = {9781450308144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993744.1993753},
doi = {10.1145/1993744.1993753},
abstract = {Cloud computing, social networking and information networks (for search, news feeds, etc) are driving interest in the deployment of large data centers. TCP is the dominant Layer 3 transport protocol in these networks. However, the operating conditions---very high bandwidth links, low round-trip times, small-buffered switches---and traffic patterns cause TCP to perform very poorly. The Data Center TCP (DCTCP) algorithm has recently been proposed as a TCP variant for data centers and addresses these shortcomings.In this paper, we provide a mathematical analysis of DCTCP. We develop a fluid model of DCTCP and use it to analyze the throughput and delay performance of the algorithm, as a function of the design parameters and of network conditions like link speeds, round-trip times and the number of active flows. Unlike fluid model representations of standard congestion control loops, the DCTCP fluid model exhibits limit cycle behavior. Therefore, it is not amenable to analysis by linearization around a fixed point and we undertake a direct analysis of the limit cycles, proving their stability. Using a hybrid (continuous- and discrete-time) model, we analyze the convergence of DCTCP sources to their fair share, obtaining an explicit characterization of the convergence rate. Finally, we investigate the "RTT-fairness" of DCTCP; i.e., the rate obtained by DCTCP sources as a function of their RTTs. We find a very simple change to DCTCP which is suggested by the fluid model and which significantly improves DCTCP's RTT-fairness. We corroborate our results with ns2 simulations.},
booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {73–84},
numpages = {12},
keywords = {data center network, congestion control, analysis, TCP},
location = {San Jose, California, USA},
series = {SIGMETRICS '11}
}

# alternate url:
# https://people.csail.mit.edu/alizadeh/papers/afqcn-hoti10.pdf
# Andy file name:
# 2010-kabbani-et-al-af-qcn-approximate-fairness-with-quantized-congestion-notification-for-multi-tenanted-data-centers.pdf
@inproceedings{KAYPP2010,
author = {Kabbani, Abdul and Alizadeh, Mohammad and Yasuda, Masato and Pan, Rong and Prabhakar, Balaji},
title = {AF-QCN: Approximate Fairness with Quantized Congestion Notification for Multi-tenanted Data Centers},
year = {2010},
isbn = {9780769542089},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HOTI.2010.26},
doi = {10.1109/HOTI.2010.26},
abstract = {Data Center Networks represent the convergence of computing and networking, of data and storage networks, and of packet transport mechanisms in Layers 2 and 3. Congestion control algorithms are a key component of data transport in this type of network. Recently, a Layer 2 congestion management algorithm, called QCN (Quantized Congestion Notification), has been adopted for the IEEE 802.1 Data Center Bridging standard: IEEE 802.1Qau. The QCN algorithm has been designed to be stable, responsive, and simple to implement. However, it does not provide weighted fairness, where the weights can be set by the operator on a per-flow or per-class basis. Such a feature can be very useful in multi-tenanted Cloud Computing and Data Center environments. This paper addresses this issue. Specifically, we develop an algorithm, called AF-QCN (for Approximately Fair QCN), which ensures a faster convergence to fairness than QCN, maintains this fairness at fine-grained time scales, and provides programmable weighted fair bandwidth shares to flows/flow-classes. It combines the QCN algorithm developed by some of the authors of this paper, and the AFD algorithm previously developed by Pan et. al. AF-QCN requires no modifications to a QCN source (Reaction Point) and introduces a very light-weight addition to a QCNcapable switch (Congestion Point). The results obtained through simulations and an FPGA implementation on a 1Gbps platform show that AF-QCN retains the good congestion management performance of QCN while achieving rapid and programmable (approximate) weighted fairness.},
booktitle = {Proceedings of the 2010 18th IEEE Symposium on High Performance Interconnects},
pages = {58–65},
numpages = {8},
series = {HOTI '10}
}

# alternate url
# https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final187.pdf
# Andy file name:
# 2012-alizadeh-et-al-less-is-more-trading-a-little-bandwidth-for-ultra-low-latency-in-the-data-center.pdf
@inproceedings{AKEPe2012,
author = {Alizadeh, Mohammad and Kabbani, Abdul and Edsall, Tom and Prabhakar, Balaji and Vahdat, Amin and Yasuda, Masato},
title = {Less is more: trading a little bandwidth for ultra-low latency in the data center},
year = {2012},
publisher = {USENIX Association},
address = {USA},
abstract = {Traditional measures of network goodness--goodput, quality of service, fairness--are expressed in terms of bandwidth. Network latency has rarely been a primary concern because delivering the highest level of bandwidth essentially entails driving up latency--at the mean and, especially, at the tail. Recently, however, there has been renewed interest in latency as a primary metric for mainstream applications. In this paper, we present the HULL (High-bandwidth Ultra-Low Latency) architecture to balance two seemingly contradictory goals: near baseline fabric latency and high bandwidth utilization. HULL leaves 'bandwidth headroom' using Phantom Queues that deliver congestion signals before network links are fully utilized and queues form at switches. By capping utilization at less than link capacity, we leave room for latency sensitive traffic to avoid buffering and the associated large delays. At the same time, we use DCTCP, a recently proposed congestion control algorithm, to adaptively respond to congestion and to mitigate the bandwidth penalties which arise from operating in a bufferless fashion. HULL further employs packet pacing to counter burstiness caused by Interrupt Coalescing and Large Send Offloading. Our implementation and simulation results show that by sacrificing a small amount (e.g., 10\%) of bandwidth, HULL can dramatically reduce average and tail latencies in the data center.},
booktitle = {Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation},
pages = {19},
numpages = {1},
location = {San Jose, CA},
series = {NSDI'12}
}

# Andy file name:
# 2012-hong-et-al-finishing-flows-quickly-with-preemptive-scheduling.pdf
@inproceedings{HCG2012,
author = {Hong, Chi-Yao and Caesar, Matthew and Godfrey, P. Brighten},
title = {Finishing flows quickly with preemptive scheduling},
year = {2012},
isbn = {9781450314190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342356.2342389},
doi = {10.1145/2342356.2342389},
abstract = {Today's data centers face extreme challenges in providing low latency. However, fair sharing, a principle commonly adopted in current congestion control protocols, is far from optimal for satisfying latency requirements.We propose Preemptive Distributed Quick (PDQ) flow scheduling, a protocol designed to complete flows quickly and meet flow deadlines. PDQ enables flow preemption to approximate a range of scheduling disciplines. For example, PDQ can emulate a shortest job first algorithm to give priority to the short flows by pausing the contending flows. PDQ borrows ideas from centralized scheduling disciplines and implements them in a fully distributed manner, making it scalable to today's data centers. Further, we develop a multipath version of PDQ to exploit path diversity.Through extensive packet-level and flow-level simulation, we demonstrate that PDQ significantly outperforms TCP, RCP and D3 in data center environments. We further show that PDQ is stable, resilient to packet loss, and preserves nearly all its performance gains even given inaccurate flow information.},
booktitle = {Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {127–138},
numpages = {12},
keywords = {data center, deadline, flow scheduling},
location = {Helsinki, Finland},
series = {SIGCOMM '12}
}

# Andy file name:
# 2012-vamanan-et-al-deadline-aware-datacenter-tc-d2tcp.pdf
@inproceedings{VHV2012,
author = {Vamanan, Balajee and Hasan, Jahangir and Vijaykumar, T.N.},
title = {Deadline-aware datacenter tcp (D2TCP)},
year = {2012},
isbn = {9781450314190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342356.2342388},
doi = {10.1145/2342356.2342388},
abstract = {An important class of datacenter applications, called Online Data-Intensive (OLDI) applications, includes Web search, online retail, and advertisement. To achieve good user experience, OLDI applications operate under soft-real-time constraints (e.g., 300 ms latency) which imply deadlines for network communication within the applications. Further, OLDI applications typically employ tree-based algorithms which, in the common case, result in bursts of children-to-parent traffic with tight deadlines. Recent work on datacenter network protocols is either deadline-agnostic (DCTCP) or is deadline-aware (D3) but suffers under bursts due to race conditions. Further, D3 has the practical drawbacks of requiring changes to the switch hardware and not being able to coexist with legacy TCP. We propose Deadline-Aware Datacenter TCP (D2TCP), a novel transport protocol, which handles bursts, is deadline-aware, and is readily deployable. In designing D2TCP, we make two contributions: (1) D2TCP uses a distributed and reactive approach for bandwidth allocation which fundamentally enables D2TCP's properties. (2) D2TCP employs a novel congestion avoidance algorithm, which uses ECN feedback and deadlines to modulate the congestion window via a gamma-correction function. Using a small-scale implementation and at-scale simulations, we show that D2TCP reduces the fraction of missed deadlines compared to DCTCP and D3 by 75\% and 50\%, respectively.},
booktitle = {Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {115–126},
numpages = {12},
keywords = {cloud services, datacenter, deadline, ecn, oldi, sla, tcp},
location = {Helsinki, Finland},
series = {SIGCOMM '12}
}

# alternate url
# https://web.stanford.edu/~skatti/pubs/sigcomm13-pfabric.pdf
# Andy file name:
# 2013-alizadeh-et-al-pfabric-minimal-near-optimal-datacenter-transport.pdf
@inproceedings{AYSKe2013,
author = {Alizadeh, Mohammad and Yang, Shuang and Sharif, Milad and Katti, Sachin and McKeown, Nick and Prabhakar, Balaji and Shenker, Scott},
title = {pFabric: minimal near-optimal datacenter transport},
year = {2013},
isbn = {9781450320566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486001.2486031},
doi = {10.1145/2486001.2486031},
abstract = {In this paper we present pFabric, a minimalistic datacenter transport design that provides near theoretically optimal flow completion times even at the 99th percentile for short flows, while still minimizing average flow completion time for long flows. Moreover, pFabric delivers this performance with a very simple design that is based on a key conceptual insight: datacenter transport should decouple flow scheduling from rate control. For flow scheduling, packets carry a single priority number set independently by each flow; switches have very small buffers and implement a very simple priority-based scheduling/dropping mechanism. Rate control is also correspondingly simpler; flows start at line rate and throttle back only under high and persistent packet loss. We provide theoretical intuition and show via extensive simulations that the combination of these two simple mechanisms is sufficient to provide near-optimal performance.},
booktitle = {Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM},
pages = {435–446},
numpages = {12},
keywords = {datacenter network, flow scheduling, packet transport},
location = {Hong Kong, China},
series = {SIGCOMM '13}
}

# alternate url
# https://2022-cs244.github.io/papers/L15-TCP-ex-machina.pdf
# Andy file name:
# 2013-winstein-et-al-tcp-ex-machina-computer-generated-congestion-control.pdf
@inproceedings{WB2013,
author = {Winstein, Keith and Balakrishnan, Hari},
title = {TCP ex machina: computer-generated congestion control},
year = {2013},
isbn = {9781450320566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486001.2486020},
doi = {10.1145/2486001.2486020},
abstract = {This paper describes a new approach to end-to-end congestion control on a multi-user network. Rather than manually formulate each endpoint's reaction to congestion signals, as in traditional protocols, we developed a program called Remy that generates congestion-control algorithms to run at the endpoints.In this approach, the protocol designer specifies their prior knowledge or assumptions about the network and an objective that the algorithm will try to achieve, e.g., high throughput and low queueing delay. Remy then produces a distributed algorithm---the control rules for the independent endpoints---that tries to achieve this objective.In simulations with ns-2, Remy-generated algorithms outperformed human-designed end-to-end techniques, including TCP Cubic, Compound, and Vegas. In many cases, Remy's algorithms also outperformed methods that require intrusive in-network changes, including XCP and Cubic-over-sfqCoDel (stochastic fair queueing with CoDel for active queue management). Remy can generate algorithms both for networks where some parameters are known tightly a priori, e.g. datacenters, and for networks where prior knowledge is less precise, such as cellular networks. We characterize the sensitivity of the resulting performance to the specificity of the prior knowledge, and the consequences when real-world conditions contradict the assumptions supplied at design-time.},
booktitle = {Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM},
pages = {123–134},
numpages = {12},
keywords = {computer-designed algorithms, congestion control},
location = {Hong Kong, China},
series = {SIGCOMM '13}
}

# Andy file name:
# 2013-winstein-et-al-stochastic-forecasts-achieve-high-throughput-and-low-delay-over-cellular-networks.pdf
@inproceedings{WSB2013,
author = {Winstein, Keith and Sivaraman, Anirudh and Balakrishnan, Hari},
title = {Stochastic forecasts achieve high throughput and low delay over cellular networks},
year = {2013},
publisher = {USENIX Association},
address = {USA},
abstract = {Sprout is an end-to-end transport protocol for interactive applications that desire high throughput and low delay. Sprout works well over cellular wireless networks, where link speeds change dramatically with time, and current protocols build up multi-second queues in network gateways. Sprout does not use TCP-style reactive congestion control; instead the receiver observes the packet arrival times to infer the uncertain dynamics of the network path. This inference is used to forecast how many bytes may be sent by the sender, while bounding the risk that packets will be delayed inside the network for too long.In evaluations on traces from four commercial LTE and 3G networks, Sprout, compared with Skype, reduced self-inflicted end-to-end delay by a factor of 7.9 and achieved 2.2\texttimes{} the transmitted bit rate on average. Compared with Google's Hangout, Sprout reduced delay by a factor of 7.2 while achieving 4.4\texttimes{} the bit rate, and compared with Apple's Facetime, Sprout reduced delay by a factor of 8.7 with 1.9\texttimes{} the bit rate.Although it is end-to-end, Sprout matched or outperformed TCP Cubic running over the CoDel active queue management algorithm, which requires changes to cellular carrier equipment to deploy. We also tested Sprout as a tunnel to carry competing interactive and bulk traffic (Skype and TCP Cubic), and found that Sprout was able to isolate client application flows from one another.},
booktitle = {Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation},
pages = {459–472},
numpages = {14},
location = {Lombard, IL},
series = {nsdi'13}
}

# alternate url
# https://people.csail.mit.edu/alizadeh/papers/conga-sigcomm14.pdf
# Andy file name:
# 2014-alizadeh-et-al-conga-distributed-congestion-aware-load-balancing-for-datacenters.pdf
@inproceedings{AEDVe2014,
author = {Alizadeh, Mohammad and Edsall, Tom and Dharmapurikar, Sarang and Vaidyanathan, Ramanan and Chu, Kevin and Fingerhut, Andy and Lam, Vinh The and Matus, Francis and Pan, Rong and Yadav, Navindra and Varghese, George},
title = {CONGA: distributed congestion-aware load balancing for datacenters},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2626316},
doi = {10.1145/2619239.2626316},
abstract = {We present the design, implementation, and evaluation of CONGA, a network-based distributed congestion-aware load balancing mechanism for datacenters. CONGA exploits recent trends including the use of regular Clos topologies and overlays for network virtualization. It splits TCP flows into flowlets, estimates real-time congestion on fabric paths, and allocates flowlets to paths based on feedback from remote switches. This enables CONGA to efficiently balance load and seamlessly handle asymmetry, without requiring any TCP modifications. CONGA has been implemented in custom ASICs as part of a new datacenter fabric. In testbed experiments, CONGA has 5x better flow completion times than ECMP even with a single link failure and achieves 2-8x better throughput than MPTCP in Incast scenarios. Further, the Price of Anarchy for CONGA is provably small in Leaf-Spine topologies; hence CONGA is nearly as effective as a centralized scheduler while being able to react to congestion in microseconds. Our main thesis is that datacenter fabric load balancing is best done in the network, and requires global schemes such as CONGA to handle asymmetry.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {503–514},
numpages = {12},
keywords = {datacenter fabric, distributed, load balancing},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

# alternate url
# http://fastpass.mit.edu/Fastpass-SIGCOMM14-Perry.pdf
# Andy file name:
# 2014-perry-et-al-fastpass-a-centralized-zero-queue-datacenter-network.pdf
@inproceedings{POBSF2014,
author = {Perry, Jonathan and Ousterhout, Amy and Balakrishnan, Hari and Shah, Devavrat and Fugal, Hans},
title = {Fastpass: a centralized "zero-queue" datacenter network},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2626309},
doi = {10.1145/2619239.2626309},
abstract = {An ideal datacenter network should provide several properties, including low median and tail latency, high utilization (throughput), fair allocation of network resources between users or applications, deadline-aware scheduling, and congestion (loss) avoidance. Current datacenter networks inherit the principles that went into the design of the Internet, where packet transmission and path selection decisions are distributed among the endpoints and routers. Instead, we propose that each sender should delegate control---to a centralized arbiter---of when each packet should be transmitted and what path it should follow.This paper describes Fastpass, a datacenter network architecture built using this principle. Fastpass incorporates two fast algorithms: the first determines the time at which each packet should be transmitted, while the second determines the path to use for that packet. In addition, Fastpass uses an efficient protocol between the endpoints and the arbiter and an arbiter replication strategy for fault-tolerant failover. We deployed and evaluated Fastpass in a portion of Facebook's datacenter network. Our results show that Fastpass achieves high throughput comparable to current networks at a 240x reduction is queue lengths (4.35 Mbytes reducing to 18 Kbytes), achieves much fairer and consistent flow throughputs than the baseline TCP (5200x reduction in the standard deviation of per-flow throughput with five concurrent connections), scalability from 1 to 8 cores in the arbiter implementation with the ability to schedule 2.21 Terabits/s of traffic in software on eight cores, and a 2.5x reduction in the number of TCP retransmissions in a latency-sensitive service at Facebook.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {307–318},
numpages = {12},
keywords = {zero-queue, scheduling, low latency, high throughput, datacenter, data plane, centralized, arbiter},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

# Andy file name:
# 2014-kabbani-et-al-flowbender-flow-level-adaptive-routing-for-improved-latency-and-throughput-in-datacenter-networks.pdf
@inproceedings{KVHD2014,
author = {Kabbani, Abdul and Vamanan, Balajee and Hasan, Jahangir and Duchene, Fabien},
title = {FlowBender: Flow-level Adaptive Routing for Improved Latency and Throughput in Datacenter Networks},
year = {2014},
isbn = {9781450332798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2674005.2674985},
doi = {10.1145/2674005.2674985},
abstract = {Datacenter networks provide high path diversity for traffic between machines. Load balancing traffic across these paths is important for both, latency- and throughput-sensitive applications. The standard load balancing techniques used today obliviously hash a flow to a random path. When long flows collide on the same path, this might lead to long lasting congestion while other paths could be underutilized, degrading performance of other flows as well. Recent proposals to address this shortcoming incur significant implementation complexity at the host that would actually slow down short flows (MPTCP), depend on relatively slow centralized controllers for rerouting large congesting flows (Hedera), or require custom switch hardware, hindering near-term deployment (DeTail).We propose FlowBender, a novel technique that: (1) Load balances distributively at the granularity of flows instead of packets, avoiding excessive packet reordering. (2) Uses end-host-driven rehashing to trigger dynamic flow-to-path assignment. (3) Recovers from link failures within a Retransmit Timeout (RTO). (4) Amounts to less than 50 lines of critical kernel code and is readily deployable in commodity data centers today. (5) Is very robust and simple to tune. We evaluate FlowBender using both simulations and a real testbed implementation, and show that it improves average and tail latencies significantly compared to state of the art techniques without incurring the significant overhead and complexity of other load balancing schemes.},
booktitle = {Proceedings of the 10th ACM International on Conference on Emerging Networking Experiments and Technologies},
pages = {149–160},
numpages = {12},
keywords = {tcp, load balancing, ecmp, data centers},
location = {Sydney, Australia},
series = {CoNEXT '14}
}

# alternate url
# https://conferences2.sigcomm.org/co-next/2015/img/papers/conext15-final1.pdf
# Andy file name:
# 2015-gao-et-al-phost-distributed-near-optimal-datacenter-transport-over-commodity-network-fabric.pdf
@inproceedings{GNKAe2015,
author = {Gao, Peter X. and Narayan, Akshay and Kumar, Gautam and Agarwal, Rachit and Ratnasamy, Sylvia and Shenker, Scott},
title = {pHost: distributed near-optimal datacenter transport over commodity network fabric},
year = {2015},
isbn = {9781450334129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2716281.2836086},
doi = {10.1145/2716281.2836086},
abstract = {The importance of minimizing flow completion times (FCT) in datacenters has led to a growing literature on new network transport designs. Of particular note is pFabric, a protocol that achieves near-optimal FCTs. However, pFabric's performance comes at the cost of generality, since pFabric requires specialized hardware that embeds a specific scheduling policy within the network fabric, making it hard to meet diverse policy goals. Aiming for generality, the recent Fastpass proposal returns to a design based on commodity network hardware and instead relies on a centralized scheduler. Fastpass achieves generality, but (as we show) loses many of pFabric's performance benefits.We present pHost, a new transport design aimed at achieving both: the near-optimal performance of pFabric and the commodity network design of Fastpass. Similar to Fastpass, pHost keeps the network simple by decoupling the network fabric from scheduling decisions. However, pHost introduces a new distributed protocol that allows end-hosts to directly make scheduling decisions, thus avoiding the overheads of Fastpass's centralized scheduler architecture. We show that pHost achieves performance on par with pFabric (within 4\% for typical conditions) and significantly outperforms Fastpass (by a factor of 3.8\texttimes{}) while relying only on commodity network hardware.},
booktitle = {Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies},
articleno = {1},
numpages = {12},
keywords = {packet transport, flow scheduling, datacenter network},
location = {Heidelberg, Germany},
series = {CoNEXT '15}
}

# Andy file name:
# 2015-bai-et-al-information-agnostic-flow-scheduling-for-commodity-data-centers.pdf
@inproceedings{BCCHe2015,
author = {Bai, Wei and Chen, Li and Chen, Kai and Han, Dongsu and Tian, Chen and Wang, Hao},
title = {Information-agnostic flow scheduling for commodity data centers},
year = {2015},
isbn = {9781931971218},
publisher = {USENIX Association},
address = {USA},
abstract = {Many existing data center network (DCN) flow scheduling schemes minimize flow completion times (FCT) based on prior knowledge of flows and custom switch functions, making them superior in performance but hard to use in practice. By contrast, we seek to minimize FCT with no prior knowledge and existing commodity switch hardware.To this end, we present PIAS, a DCN flow scheduling mechanism that aims to minimize FCT by mimicking Shortest Job First (SJF) on the premise that flow size is not known a priori. At its heart, PIAS leverages multiple priority queues available in existing commodity switches to implement a Multiple Level Feedback Queue (MLFQ), in which a PIAS flow is gradually demoted from higher-priority queues to lower-priority queues based on the number of bytes it has sent. As a result, short flows are likely to be finished in the first few high-priority queues and thus be prioritized over long flows in general, which enables PIAS to emulate SJF without knowing flow sizes beforehand.We have implemented a PIAS prototype and evaluated PIAS through both testbed experiments and ns- 2 simulations. We show that PIAS is readily deployable with commodity switches and backward compatible with legacy TCP/IP stacks. Our evaluation results show that PIAS significantly outperforms existing information-agnostic schemes. For example, it reduces FCT by up to 50\% and 40\% over DCTCP [11] and L2DCT [27] respectively; and it only has a 4.9\% performance gap to an ideal information-aware scheme, pFabric [13], for short flows under a production DCN workload.},
booktitle = {Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation},
pages = {455–468},
numpages = {14},
location = {Oakland, CA},
series = {NSDI'15}
}

# Andy file name:
# 2015-grosvenor-et-al-queues-dont-matter-when-you-can-jump-them.pdf
@inproceedings{GSGWe2015,
author = {Grosvenor, Matthew P. and Schwarzkopf, Malte and Gog, Ionel and Watson, Robert N. M. and Moore, Andrew W. and Hand, Steven and Crowcroft, Jon},
title = {Queues don't matter when you can JUMP them!},
year = {2015},
isbn = {9781931971218},
publisher = {USENIX Association},
address = {USA},
abstract = {QJUMP is a simple and immediately deployable approach to controlling network interference in datacenter networks. Network interference occurs when congestion from throughput-intensive applications causes queueing that delays traffic from latency-sensitive applications. To mitigate network interference, QJUMP applies Internet QoS-inspired techniques to datacenter applications. Each application is assigned to a latency sensitivity level (or class). Packets from higher levels are rate-limited in the end host, but once allowed into the network can "jump-the-queue" over packets from lower levels. In settings with known node counts and link speeds, QJUMP can support service levels ranging from strictly bounded latency (but with low rate) through to line-rate throughput (but with high latency variance).We have implemented QJUMP as a Linux Traffic Control module. We show that QJUMP achieves bounded latency and reduces in-network interference by up to 300\texttimes{}, outperforming Ethernet Flow Control (802.3x), ECN (WRED) and DCTCP. We also show that QJUMP improves average flow completion times, performing close to or better than DCTCP and pFabric.},
booktitle = {Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation},
pages = {1–14},
numpages = {14},
location = {Oakland, CA},
series = {NSDI'15}
}

# alternate url
# https://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p465.pdf
# Andy file name:
# 2015-he-et-al-presto-edge-based-load-balancing-for-fast-datacenter-networks.pdf
@inproceedings{HRAFe2015,
author = {He, Keqiang and Rozner, Eric and Agarwal, Kanak and Felter, Wes and Carter, John and Akella, Aditya},
title = {Presto: Edge-based Load Balancing for Fast Datacenter Networks},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787507},
doi = {10.1145/2785956.2787507},
abstract = {Datacenter networks deal with a variety of workloads, ranging from latency-sensitive small flows to bandwidth-hungry large flows. Load balancing schemes based on flow hashing, e.g., ECMP, cause congestion when hash collisions occur and can perform poorly in asymmetric topologies. Recent proposals to load balance the network require centralized traffic engineering, multipath-aware transport, or expensive specialized hardware. We propose a mechanism that avoids these limitations by (i) pushing load-balancing functionality into the soft network edge (e.g., virtual switches) such that no changes are required in the transport layer, customer VMs, or networking hardware, and (ii) load balancing on fine-grained, near-uniform units of data (flowcells) that fit within end-host segment offload optimizations used to support fast networking speeds. We design and implement such a soft-edge load balancing scheme, called Presto, and evaluate it on a 10 Gbps physical testbed. We demonstrate the computational impact of packet reordering on receivers and propose a mechanism to handle reordering in the TCP receive offload functionality. Presto's performance closely tracks that of a single, non-blocking switch over many workloads and is adaptive to failures and topology asymmetry.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {465–478},
numpages = {14},
keywords = {load balancing, software-defined networking},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

# Andy file name:
# 2015-mittal-et-al-timely-rtt-based-congestion-control-for-the-datacenter.pdf
@inproceedings{MLDBe2015,
author = {Mittal, Radhika and Lam, Vinh The and Dukkipati, Nandita and Blem, Emily and Wassel, Hassan and Ghobadi, Monia and Vahdat, Amin and Wang, Yaogong and Wetherall, David and Zats, David},
title = {TIMELY: RTT-based Congestion Control for the Datacenter},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787510},
doi = {10.1145/2785956.2787510},
abstract = {Datacenter transports aim to deliver low latency messaging together with high throughput. We show that simple packet delay, measured as round-trip times at hosts, is an effective congestion signal without the need for switch feedback. First, we show that advances in NIC hardware have made RTT measurement possible with microsecond accuracy, and that these RTTs are sufficient to estimate switch queueing. Then we describe how TIMELY can adjust transmission rates using RTT gradients to keep packet latency low while delivering high bandwidth. We implement our design in host software running over NICs with OS-bypass capabilities. We show using experiments with up to hundreds of machines on a Clos network topology that it provides excellent performance: turning on TIMELY for OS-bypass messaging over a fabric with PFC lowers 99 percentile tail latency by 9X while maintaining near line-rate throughput. Our system also outperforms DCTCP running in an optimized kernel, reducing tail latency by $13$X. To the best of our knowledge, TIMELY is the first delay-based congestion control protocol for use in the datacenter, and it achieves its results despite having an order of magnitude fewer RTT signals (due to NIC offload) than earlier delay-based schemes such as Vegas.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {537–550},
numpages = {14},
keywords = {rdma, os-bypass, delay-based congestion control, datacenter transport},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

# Andy file name:
# 2015-dong-et-al-pcc-re-architecting-congestion-control-for-consistent-high-performance.pdf
@inproceedings{DLZGS2015,
author = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, P. Brighten and Schapira, Michael},
title = {PCC: re-architecting congestion control for consistent high performance},
year = {2015},
isbn = {9781931971218},
publisher = {USENIX Association},
address = {USA},
abstract = {TCP and its variants have suffered from surprisingly poor performance for decades. We argue the TCP family has little hope of achieving consistent high performance due to a fundamental architectural deficiency: hardwiring packet-level events to control responses. We propose Performance-oriented Congestion Control (PCC), a new congestion control architecture in which each sender continuously observes the connection between its actions and empirically experienced performance, enabling it to consistently adopt actions that result in high performance. We prove that PCC converges to a stable and fair equilibrium. Across many real-world and challenging environments, PCC shows consistent and often 10\texttimes{} performance improvement, with better fairness and stability than TCP. PCC requires no router hardware support or new packet format.},
booktitle = {Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation},
pages = {395–408},
numpages = {14},
location = {Oakland, CA},
series = {NSDI'15}
}

# alternate url
# https://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p523.pdf
# Andy file name:
# 2015-zhu-et-al-congestion-control-for-large-scale-rdma-deployments.pdf
@inproceedings{ZEFGe2015,
author = {Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming},
title = {Congestion Control for Large-Scale RDMA Deployments},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787484},
doi = {10.1145/2785956.2787484},
abstract = {Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (&lt; 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {523–536},
numpages = {14},
keywords = {datacenter transport, congestion control, RDMA, PFC, ECN},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

# Andy file name:
# 2015-lee-et-al-accurate-latency-based-congestion-feedback-for-datacenters.pdf
@inproceedings{LPJMH2015,
author = {Lee, Changhyun and Park, Chunjong and Jang, Keon and Moon, Sue and Han, Dongsu},
title = {Accurate latency-based congestion feedback for datacenters},
year = {2015},
isbn = {9781931971225},
publisher = {USENIX Association},
address = {USA},
abstract = {The nature of congestion feedback largely governs the behavior of congestion control. In datacenter networks, where RTTs are in hundreds of microseconds, accurate feedback is crucial to achieve both high utilization and low queueing delay. Proposals for datacenter congestion control predominantly leverage ECN or even explicit in-network feedback (e.g., RCP-type feedback) to minimize the queuing delay. In this work we explore latency-based feedback as an alternative and show its advantages over ECN. Against the common belief that such implicit feed-back is noisy and inaccurate, we demonstrate that latency-based implicit feedback is accurate enough to signal a single packet's queuing delay in 10 Gbps networks.DX enables accurate queuing delay measurements whose error falls within 1.98 and 0.53 microseconds using software-based and hardware-based latency measurements, respectively. This enables us to design a new congestion control algorithm that performs fine-grained control to adjust the congestion window just enough to achieve very low queuing delay while attaining full utilization. Our extensive evaluation shows that 1) the latency measurement accurately reflects the one-way queuing delay in single packet level; 2) the latency feedback can be used to perform practical and fine-grained congestion control in high-speed datacenter networks; and 3) DX outperforms DCTCP with 5.33\texttimes{} smaller median queueing delay at 1 Gbps and 1.57\texttimes{} at 10 Gbps.},
booktitle = {Proceedings of the 2015 USENIX Conference on Usenix Annual Technical Conference},
pages = {403–415},
numpages = {13},
location = {Santa Clara, CA},
series = {USENIX ATC '15}
}

# Andy file name:
# 2016-chen-et-al-scheduling-mix-flows-in-commodity-datacenters-with-karuna.pdf
@inproceedings{CCBA2016,
author = {Chen, Li and Chen, Kai and Bai, Wei and Alizadeh, Mohammad},
title = {Scheduling Mix-flows in Commodity Datacenters with Karuna},
year = {2016},
isbn = {9781450341936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934872.2934888},
doi = {10.1145/2934872.2934888},
abstract = {Cloud applications generate a mix of flows with and without deadlines. Scheduling such mix-flows is a key challenge; our experiments show that trivially combining existing schemes for deadline/non-deadline flows is problematic. For example, prioritizing deadline flows hurts flow completion time (FCT) for non-deadline flows, with minor improvement for deadline miss rate.We present Karuna, a first systematic solution for scheduling mix-flows. Our key insight is that deadline flows should meet their deadlines while minimally impacting the FCT of non-deadline flows. To achieve this goal, we design a novel Minimal-impact Congestion control Protocol (MCP) that handles deadline flows with as little bandwidth as possible. For non-deadline flows, we extend an existing FCT minimization scheme to schedule flows with known and unknown sizes. Karuna requires no switch modifications and is back- ward compatible with legacy TCP/IP stacks. Our testbed experiments and simulations show that Karuna effectively schedules mix-flows, for example, reducing the 95th percentile FCT of non-deadline flows by up to 47.78\% at high load compared to pFabric, while maintaining low (<5.8\%) deadline miss rate.},
booktitle = {Proceedings of the 2016 ACM SIGCOMM Conference},
pages = {174–187},
numpages = {14},
keywords = {Flow scheduling, Deadline, Datacenter networks},
location = {Florianopolis, Brazil},
series = {SIGCOMM '16}
}

# Andy file name:
# 2016-cardwell-et-al-bbr-congestion-based-congestion-control-measuring-bottleneck-bandwidth-and-round-trip-propagation-time.pdf
@article{CCGYJ2016,
author = {Cardwell, Neal and Cheng, Yuchung and Gunn, C. Stephen and Yeganeh, Soheil Hassas and Jacobson, Van},
title = {BBR: Congestion-Based Congestion Control: Measuring bottleneck bandwidth and round-trip propagation time},
year = {2016},
issue_date = {September-October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1542-7730},
url = {https://doi.org/10.1145/3012426.3022184},
doi = {10.1145/3012426.3022184},
abstract = {When bottleneck buffers are large, loss-based congestion control keeps them full, causing bufferbloat. When bottleneck buffers are small, loss-based congestion control misinterprets loss as a signal of congestion, leading to low throughput. Fixing these problems requires an alternative to loss-based congestion control. Finding this alternative requires an understanding of where and how network congestion originates.},
journal = {Queue},
month = {oct},
pages = {20–53},
numpages = {34}
}

# Andy file name:
# 2017-handley-et-al-re-architecting-datacenter-networks-and-stacks-for-low-latency-and-high-performance.pdf
@inproceedings{HRAVe2017,
author = {Handley, Mark and Raiciu, Costin and Agache, Alexandru and Voinescu, Andrei and Moore, Andrew W. and Antichi, Gianni and W\'{o}jcik, Marcin},
title = {Re-architecting datacenter networks and stacks for low latency and high performance},
year = {2017},
isbn = {9781450346535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098822.3098825},
doi = {10.1145/3098822.3098825},
abstract = {Modern datacenter networks provide very high capacity via redundant Clos topologies and low switch latency, but transport protocols rarely deliver matching performance. We present NDP, a novel data-center transport architecture that achieves near-optimal completion times for short transfers and high flow throughput in a wide range of scenarios, including incast. NDP switch buffers are very shallow and when they fill the switches trim packets to headers and priority forward the headers. This gives receivers a full view of instantaneous demand from all senders, and is the basis for our novel, high-performance, multipath-aware transport protocol that can deal gracefully with massive incast events and prioritize traffic from different senders on RTT timescales. We implemented NDP in Linux hosts with DPDK, in a software switch, in a NetFPGA-based hardware switch, and in P4. We evaluate NDP's performance in our implementations and in large-scale simulations, simultaneously demonstrating support for very low-latency and high throughput.},
booktitle = {Proceedings of the Conference of the ACM Special Interest Group on Data Communication},
pages = {29–42},
numpages = {14},
keywords = {Datacenters, Network Stacks, Transport Protocols},
location = {Los Angeles, CA, USA},
series = {SIGCOMM '17}
}

# Andy file name:
# 2017-cho-et-al-credit-scheduled-delay-bounded-congestion-control-for-datacenters.pdf
@inproceedings{CJH2017,
author = {Cho, Inho and Jang, Keon and Han, Dongsu},
title = {Credit-Scheduled Delay-Bounded Congestion Control for Datacenters},
year = {2017},
isbn = {9781450346535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098822.3098840},
doi = {10.1145/3098822.3098840},
abstract = {Small RTTs (~tens of microseconds), bursty flow arrivals, and a large number of concurrent flows (thousands) in datacenters bring fundamental challenges to congestion control as they either force a flow to send at most one packet per RTT or induce a large queue build-up. The widespread use of shallow buffered switches also makes the problem more challenging with hosts generating many flows in bursts. In addition, as link speeds increase, algorithms that gradually probe for bandwidth take a long time to reach the fair-share. An ideal datacenter congestion control must provide 1) zero data loss, 2) fast convergence, 3) low buffer occupancy, and 4) high utilization. However, these requirements present conflicting goals.This paper presents a new radical approach, called ExpressPass, an end-to-end credit-scheduled, delay-bounded congestion control for datacenters. ExpressPass uses credit packets to control congestion even before sending data packets, which enables us to achieve bounded delay and fast convergence. It gracefully handles bursty flow arrivals. We implement ExpressPass using commodity switches and provide evaluations using testbed experiments and simulations. ExpressPass converges up to 80 times faster than DCTCP in 10 Gbps links, and the gap increases as link speeds become faster. It greatly improves performance under heavy incast workloads and significantly reduces the flow completion times, especially, for small and medium size flows compared to RCP, DCTCP, HULL, and DX under realistic workloads.},
booktitle = {Proceedings of the Conference of the ACM Special Interest Group on Data Communication},
pages = {239–252},
numpages = {14},
keywords = {Datacenter Network, Credit-based, Congestion Control},
location = {Los Angeles, CA, USA},
series = {SIGCOMM '17}
}

# Andy file name:
# 2017-arashloo-et-al-hotcocoa-hardware-congestion-control-abstractions.pdf
@inproceedings{AGRW2017,
author = {Arashloo, Mina Tahmasbi and Ghobadi, Monia and Rexford, Jennifer and Walker, David},
title = {HotCocoa: Hardware Congestion Control Abstractions},
year = {2017},
isbn = {9781450355698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152434.3152457},
doi = {10.1145/3152434.3152457},
abstract = {Congestion control in multi-tenant data centers is an active area of research because of its significant impact on customer experience, and, consequently, on revenue. Therefore, new algorithms and protocols are expected to emerge as the Cloud evolves. Deploying new congestion control algorithms in the end host's hypervisor allows frequent updates, but processing packets at high rates in the hypervisor and implementing the elements of a congestion control algorithm, such as traffic shapers and timestamps, in software have well-studied inaccuracies and CPU inefficiencies. In this paper, we argue for implementing the entire congestion control algorithm in programmable NICs. To do so, we identify the absence of hardware-aware programming abstractions as the most immediate challenge and solve it using a simple high-level domain specific language called HotCocoa. HotCocoa lies at a sweet spot between the ability to express a broad set of congestion control algorithms and efficient hardware implementation. It offers a set of hardware-aware COngestion COntrol Abstractions that enable operators to specify their algorithm without having to worry about low-level hardware primitives. To evaluate HotCocoa, we implement four congestion control algorithms (Reno, DCTCP, PCC, and TIMELY) and use simulations to show that HotCocoa's implementation of Reno perfectly tracks the behavior of a native implementation in C++.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
pages = {108–114},
numpages = {7},
location = {<conf-loc>, <city>Palo Alto</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {HotNets '17}
}

# Andy file name:
# 2017-goyal-et-al-rethinking-congestion-control-for-cellular-networks.pdf
@inproceedings{GAB2017,
author = {Goyal, Prateesh and Alizadeh, Mohammad and Balakrishnan, Hari},
title = {Rethinking Congestion Control for Cellular Networks},
year = {2017},
isbn = {9781450355698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152434.3152437},
doi = {10.1145/3152434.3152437},
abstract = {We propose Accel-Brake Control (ABC), a protocol that integrates a simple and deployable signaling scheme at cellular base stations with an endpoint mechanism to respond to these signals. The key idea is for the base station to enable each sender to achieve a computed target rate by marking each packet with an "accelerate" or "brake" notification, which causes the sender to either slightly increase or slightly reduce its congestion window. ABC is designed to rapidly acquire any capacity that opens up, a common occurrence in cellular networks, while responding promptly to congestion. It is also incrementally deployable using existing ECN infrastructure and can co-exist with legacy ECN routers. Preliminary results obtained over cellular network traces show that ABC outperforms prior approaches significantly.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
pages = {29–35},
numpages = {7},
location = {<conf-loc>, <city>Palo Alto</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {HotNets '17}
}

# Andy file name:
# 2018-montazeri-et-al-homa-a-receiver-driven-low-latency-transport-protocol-using-network-priorities.pdf
@inproceedings{MLAO2018,
author = {Montazeri, Behnam and Li, Yilong and Alizadeh, Mohammad and Ousterhout, John},
title = {Homa: a receiver-driven low-latency transport protocol using network priorities},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230564},
doi = {10.1145/3230543.3230564},
abstract = {Homa is a new transport protocol for datacenter networks. It provides exceptionally low latency, especially for workloads with a high volume of very short messages, and it also supports large messages and high network utilization. Homa uses in-network priority queues to ensure low latency for short messages; priority allocation is managed dynamically by each receiver and integrated with a receiver-driven flow control mechanism. Homa also uses controlled overcommitment of receiver downlinks to ensure efficient bandwidth utilization at high load. Our implementation of Homa delivers 99th percentile round-trip times less than 15 μs for short messages on a 10 Gbps network running at 80\% load. These latencies are almost 100x lower than the best published measurements of an implementation. In simulations, Homa's latency is roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for almost all message sizes and workloads. Homa can also sustain higher network loads than pFabric, pHost, or PIAS.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {221–235},
numpages = {15},
keywords = {data centers, low latency, network stacks, transport protocols},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

# Andy file name:
# 2018-narayan-et-al-restructuring-endpoint-congestion-control.pdf
@inproceedings{NCRGe2018,
author = {Narayan, Akshay and Cangialosi, Frank and Raghavan, Deepti and Goyal, Prateesh and Narayana, Srinivas and Mittal, Radhika and Alizadeh, Mohammad and Balakrishnan, Hari},
title = {Restructuring endpoint congestion control},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230553},
doi = {10.1145/3230543.3230553},
abstract = {This paper describes the implementation and evaluation of a system to implement complex congestion control functions by placing them in a separate agent outside the datapath. Each datapath---such as the Linux kernel TCP, UDP-based QUIC, or kernel-bypass transports like mTCP-on-DPDK---summarizes information about packet round-trip times, receptions, losses, and ECN via a well-defined interface to algorithms running in the off-datapath Congestion Control Plane (CCP). The algorithms use this information to control the datapath's congestion window or pacing rate. Algorithms written in CCP can run on multiple datapaths. CCP improves both the pace of development and ease of maintenance of congestion control algorithms by providing better, modular abstractions, and supports aggregation capabilities of the Congestion Manager, all with one-time changes to datapaths. CCP also enables new capabilities, such as Copa in Linux TCP, several algorithms running on QUIC and mTCP/DPDK, and the use of signal processing algorithms to detect whether cross-traffic is ACK-clocked. Experiments with our user-level Linux CCP implementation show that CCP algorithms behave similarly to kernel algorithms, and incur modest CPU overhead of a few percent.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {30–43},
numpages = {14},
keywords = {operating systems, congestion control},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

# alternate url
# https://dspace.mit.edu/bitstream/handle/1721.1/137381/1802.08730.pdf
# Andy file name:
# 2018-goyal-et-al-elasticity-detection-a-building-block-for-delay-sensitive-congestion-control.pdf
@inproceedings{GNCRe2018,
author = {Goyal, Prateesh and Narayan, Akshay and Cangialosi, Frank and Raghavan, Deepti and Narayana, Srinivas and Alizadeh, Mohammad and Balakrishnan, Hari},
title = {Elasticity Detection: A Building Block for Delay-Sensitive Congestion Control},
year = {2018},
isbn = {9781450355858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3232755.3232772},
doi = {10.1145/3232755.3232772},
abstract = {This paper develops a technique to detect whether the cross traffic competing with a flow is elastic or not, and shows how to use the elasticity detector to improve congestion control. If the cross traffic is elastic, i.e., made up of buffer-filling flows like Cubic or Reno, then one should use a scheme that competes well with such traffic. Such a scheme will not be able to control delays because the cross traffic will not cooperate. If, however, cross traffic is inelastic, then one can use a suitable delay-sensitive congestion control algorithm, which can control delays, but which would have obtained dismal throughput when run concurrently with a buffer-filling algorithm.We use the elasticity detector to demonstrate a congestion control framework that always achieves high utilization, but which can also achieve low delays when cross traffic permits it. The technique uses an asymmetric sinusoidal pulse patternand estimates elasticity by computing the frequency response(FFT) of the cross traffic estimate; we have measured its accuracy to be over 90\%. We have developed Nimbus, a protocol that explicitly switches between TCP-competitive and delay-sensitive modes using the elasticity detector. Our results on emulated and real-world paths show that Nimbus achieves throughput comparable to or better than Cubic always, but with delays that are much lower when cross traffic is inelastic. Unlike BBR, Nimbus is fair to Cubic, and has significantly lower delay in all cases; for example, on real-world paths, Nimbus has 11\% lower throughput but at 40-50 ms lower packet delay.},
booktitle = {Proceedings of the Applied Networking Research Workshop},
pages = {75},
numpages = {1},
location = {Montreal, QC, Canada},
series = {ANRW '18}
}

# alternate url
# https://www.usenix.org/system/files/conference/atc18/atc18-yan-francis.pdf
# Andy file name:
# 2018-yan-et-al-pantheon-the-training-ground-for-internet-congestion-control-research.pdf
@inproceedings{YMHRe2018,
author = {Yan, Francis Y. and Ma, Jestin and Hill, Greg D. and Raghavan, Deepti and Wahby, Riad S. and Levis, Philip and Winstein, Keith},
title = {Pantheon: the training ground for internet congestion-control research},
year = {2018},
isbn = {9781931971447},
publisher = {USENIX Association},
address = {USA},
abstract = {Internet transport algorithms are foundational to the performance of network applications. But a number of practical challenges make it difficult to evaluate new ideas and algorithms in a reproducible manner. We present the Pantheon, a system that addresses this by serving as a community "training ground" for research on Internet transport protocols and congestion control (https://pantheon.stanford.edu). It allows network researchers to benefit from and contribute to a common set of benchmark algorithms, a shared evaluation platform, and a public archive of results.We present three results showing the Pantheon's value as a research tool. First, we describe a measurement study from more than a year of data, indicating that congestion-control schemes vary dramatically in their relative performance as a function of path dynamics. Second, the Pantheon generates calibrated network emulators that capture the diverse performance of real Internet paths. These enable reproducible and rapid experiments that closely approximate real-world results. Finally, we describe the Pantheon's contribution to developing new congestion-control schemes, two of which were published at USENIX NSDI 2018, as well as data-driven neural-network-based congestion-control schemes that can be trained to achieve good performance over the real Internet.},
booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
pages = {731–743},
numpages = {13},
location = {Boston, MA, USA},
series = {USENIX ATC '18}
}

# alternate url
# https://www.usenix.org/system/files/conference/nsdi18/nsdi18-arun.pdf
# Andy file name:
# 2018-arun-et-al-copa-practical-delay-based-congestion-control-for-the-internet.pdf
@inproceedings {AB2018,
	author = {Venkat Arun and Hari Balakrishnan},
	title = {Copa: Congestion Control Combining Objective Optimization with Window Adjustments},
	booktitle = {15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18)},
	year = {2018},
	address = {Renton, WA},
	url = {https://www.usenix.org/conference/nsdi18/presentation/arun},
	publisher = {{USENIX} Association},
	month = apr
}

# Andy file name:
# 2021-arun-et-al-toward-formally-verifying-congestion-control-behavior.pdf
@inproceedings{AASAB2021,
author = {Arun, Venkat and Arashloo, Mina Tahmasbi and Saeed, Ahmed and Alizadeh, Mohammad and Balakrishnan, Hari},
title = {Toward formally verifying congestion control behavior},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472912},
doi = {10.1145/3452296.3472912},
abstract = {The diversity of paths on the Internet makes it difficult for designers and operators to confidently deploy new congestion control algorithms (CCAs) without extensive real-world experiments, but such capabilities are not available to most of the networking community. And even when they are available, understanding why a CCA underperforms by trawling through massive amounts of statistical data from network connections is challenging. The history of congestion control is replete with many examples of surprising and unanticipated behaviors unseen in simulation but observed on real-world paths. In this paper, we propose initial steps toward modeling and improving our confidence in a CCA's behavior. We have developed CCAC, a tool that uses formal verification to establish certain properties of CCAs. It is able to prove hypotheses about CCAs or generate counterexamples for invalid hypotheses. With CCAC, a designer can not only gain greater confidence prior to deployment to avoid unpleasant surprises, but can also use the counterexamples to iteratively improvetheir algorithm. We have modeled additive-increase/multiplicative-decrease (AIMD), Copa, and BBR with CCAC, and describe some surprising results from the exercise.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {1–16},
numpages = {16},
keywords = {formal verification, congestion control, WAN transport},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

# Andy file name:
# 1998-stoica-et-al-core-stateless-fair-queueing-achieving-approximately-fair-bandwidth-allocations-in-high-speed-networks.pdf
@inproceedings{SSZ1998,
author = {Stoica, Ion and Shenker, Scott and Zhang, Hui},
title = {Core-stateless fair queueing: achieving approximately fair bandwidth allocations in high speed networks},
year = {1998},
isbn = {1581130031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/285237.285273},
doi = {10.1145/285237.285273},
abstract = {Router mechanisms designed to achieve fair bandwidth allocations, like Fair Queueing, have many desirable properties for congestion control in the Internet. However, such mechanisms usually need to maintain state, manage buffers, and/or perform packet scheduling on a per flow basis, and this complexity may prevent them from being cost-effectively implemented and widely deployed. In this paper, we propose an architecture that significantly reduces this implementation complexity yet still achieves approximately fair bandwidth allocations. We apply this approach to an island of routers --- that is, a contiguous region of the network --- and we distinguish between edge routers and core routers. Edge routers maintain per flow state; they estimate the incoming rate of each flow and insert a label into each packet header based on this estimate. Core routers maintain no per flow state; they use FIFO packet scheduling augmented by a probabilistic dropping algorithm that uses the packet labels and an estimate of the aggregate traffic at the router. We call the scheme Core-Stateless Fair Queueing. We present simulations and analysis on the performance of this approach, and discuss an alternate approach.},
booktitle = {Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {118–130},
numpages = {13},
location = {Vancouver, British Columbia, Canada},
series = {SIGCOMM '98}
}

# alternate url
# https://www.scss.tcd.ie/Doug.Leith/pubs/pfldnet2007_cubic_final.pdf
# Andy file name:
# 2008-leith-et-al-experimental-evaluation-of-cubic-tcp.pdf
@inproceedings{LSM2008,
       booktitle = {Proceedings of the 6th International Workshop on Protocols for Fast Long-Distance Networks (PFLDnet 2008) },
           title = {Experimental evaluation of Cubic-TCP},
          author = {Douglas J. Leith and Robert N. Shorten and G. McCullagh},
            year = {2008},
        keywords = {Cubic TCP algorithm; Transmission control protocol; Fairness; Networks; PFLDnet 2008; Hamilton Institute.},
             url = {https://mural.maynoothuniversity.ie/1716/},
        abstract = {In this paper we present an initial experimental evaluation of the recently proposed Cubic-TCP algorithm. Results are presented using a suite of benchmark tests that have been recently proposed in the literature [12], and a number of
issues are of practical concern highlighted.}
}

# Andy file name:
# 1997-mathis-et-al-the-macroscopic-behavior-of-the-tcp-congestion-avoidance-algorithm.pdf
@article{MSMO1997,
author = {Mathis, Matthew and Semke, Jeffrey and Mahdavi, Jamshid and Ott, Teunis},
title = {The macroscopic behavior of the TCP congestion avoidance algorithm},
year = {1997},
issue_date = {July 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/263932.264023},
doi = {10.1145/263932.264023},
abstract = {In this paper, we analyze a performance model for the TCP Congestion Avoidance algorithm. The model predicts the bandwidth of a sustained TCP connection subjected to light to moderate packet losses, such as loss caused by network congestion. It assumes that TCP avoids retransmission timeouts and always has sufficient receiver window and sender data. The model predicts the Congestion Avoidance performance of nearly all TCP implementations under restricted conditions and of TCP with Selective Acknowledgements over a much wider range of Internet conditions.We verify the model through both simulation and live Internet measurements. The simulations test several TCP implementations under a range of loss conditions and in environments with both drop-tail and RED queuing. The model is also compared to live Internet measurements using the TReno diagnostic and real TCP implementations.We also present several applications of the model to problems of bandwidth allocation in the Internet. We use the model to analyze networks with multiple congested gateways; this analysis shows strong agreement with prior work in this area. Finally, we present several important implications about the behavior of the Internet in the presence of high load from diverse user communities.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jul},
pages = {67–82},
numpages = {16}
}

# Andy file name:
# 2017-vanini-et-al-let-it-flow-resilient-asymmetric-load-balancing-with-flowlet-switching.pdf
@inproceedings{VPATE2017,
author = {Vanini, Erico and Pan, Rong and Alizadeh, Mohammad and Taheri, Parvin and Edsall, Tom},
title = {Let it flow: resilient asymmetric load balancing with flowlet switching},
year = {2017},
isbn = {9781931971379},
publisher = {USENIX Association},
address = {USA},
abstract = {Datacenter networks require efficient multi-path load balancing to achieve high bisection bandwidth. Despite much progress in recent years towards addressing this challenge, a load balancing design that is both simple to implement and resilient to network asymmetry has remained elusive. In this paper, we show that flowlet switching, an idea first proposed more than a decade ago, is a powerful technique for resilient load balancing with asymmetry. Flowlets have a remarkable elasticity property: their size changes automatically based on traffic conditions on their path. We use this insight to develop LetFlow, a very simple load balancing scheme that is resilient to asymmetry. LetFlow simply picks paths at random for flowlets and lets their elasticity naturally balance the traffic on different paths. Our extensive evaluation with real hardware and packet-level simulations shows that LetFlow is very effective. Despite being much simpler, it performs significantly better than other traffic oblivious schemes like WCMP and Presto in asymmetric scenarios, while achieving average flow completions time within 10-20\% of CONGA in testbed experiments and 2\texttimes{} of CONGA in simulated topologies with large asymmetry and heavy traffic load.},
booktitle = {Proceedings of the 14th USENIX Conference on Networked Systems Design and Implementation},
pages = {407–420},
numpages = {14},
location = {Boston, MA, USA},
series = {NSDI'17}
}

# Andy file name:
# 2017-zhang-et-al-resilient-datacenter-load-balancing-in-the-wild.pdf
@inproceedings{ZZBCC2017,
author = {Zhang, Hong and Zhang, Junxue and Bai, Wei and Chen, Kai and Chowdhury, Mosharaf},
title = {Resilient Datacenter Load Balancing in the Wild},
year = {2017},
isbn = {9781450346535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098822.3098841},
doi = {10.1145/3098822.3098841},
abstract = {Production datacenters operate under various uncertainties such as traffic dynamics, topology asymmetry, and failures. Therefore, datacenter load balancing schemes must be resilient to these uncertainties; i.e., they should accurately sense path conditions and timely react to mitigate the fallouts. Despite significant efforts, prior solutions have important drawbacks. On the one hand, solutions such as Presto and DRB are oblivious to path conditions and blindly reroute at fixed granularity. On the other hand, solutions such as CONGA and CLOVE can sense congestion, but they can only reroute when flowlets emerge; thus, they cannot always react timely to uncertainties. To make things worse, these solutions fail to detect/handle failures such as blackholes and random packet drops, which greatly degrades their performance.In this paper, we introduce Hermes, a datacenter load balancer that is resilient to the aforementioned uncertainties. At its heart, Hermes leverages comprehensive sensing to detect path conditions including failures unattended before, and it reacts using timely yet cautious rerouting. Hermes is a practical edge-based solution with no switch modification. We have implemented Hermes with commodity switches and evaluated it through both testbed experiments and large-scale simulations. Our results show that Hermes achieves comparable performance to CONGA and Presto in normal cases, and well handles uncertainties: under asymmetries, Hermes achieves up to 10\% and 20\% better flow completion time (FCT) than CONGA and CLOVE; under switch failures, it outperforms all other schemes by over 32\%.},
booktitle = {Proceedings of the Conference of the ACM Special Interest Group on Data Communication},
pages = {253–266},
numpages = {14},
keywords = {Datacenter fabric, Distributed, Load balancing},
location = {Los Angeles, CA, USA},
series = {SIGCOMM '17}
}

# Andy file name:
# 2019-li-et-al-hpcc-high-precision-congestion-control.pdf
@inproceedings{LMLZe2019,
author = {Li, Yuliang and Miao, Rui and Liu, Hongqiang Harry and Zhuang, Yan and Feng, Fei and Tang, Lingbo and Cao, Zheng and Zhang, Ming and Kelly, Frank and Alizadeh, Mohammad and Yu, Minlan},
title = {HPCC: high precision congestion control},
year = {2019},
isbn = {9781450359566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341302.3342085},
doi = {10.1145/3341302.3342085},
abstract = {Congestion control (CC) is the key to achieving ultra-low latency, high bandwidth and network stability in high-speed networks. From years of experience operating large-scale and high-speed RDMA networks, we find the existing high-speed CC schemes have inherent limitations for reaching these goals. In this paper, we present HPCC (High Precision Congestion Control), a new high-speed CC mechanism which achieves the three goals simultaneously. HPCC leverages in-network telemetry (INT) to obtain precise link load information and controls traffic precisely. By addressing challenges such as delayed INT information during congestion and overreac-tion to INT information, HPCC can quickly converge to utilize free bandwidth while avoiding congestion, and can maintain near-zero in-network queues for ultra-low latency. HPCC is also fair and easy to deploy in hardware. We implement HPCC with commodity programmable NICs and switches. In our evaluation, compared to DCQCN and TIMELY, HPCC shortens flow completion times by up to 95\%, causing little congestion even under large-scale incasts.},
booktitle = {Proceedings of the ACM Special Interest Group on Data Communication},
pages = {44–58},
numpages = {15},
keywords = {smart NIC, programmable switch, congestion control, RDMA},
location = {Beijing, China},
series = {SIGCOMM '19}
}

# alternate url
# https://www.cs.cornell.edu/~ragarwal/pubs/network-stack.pdf
# Andy file name:
# 2021-cai-et-al-understanding-host-network-stack-overheads.pdf
@inproceedings{CCVHA2021,
author = {Cai, Qizhe and Chaudhary, Shubham and Vuppalapati, Midhul and Hwang, Jaehyun and Agarwal, Rachit},
title = {Understanding host network stack overheads},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472888},
doi = {10.1145/3452296.3472888},
abstract = {Traditional end-host network stacks are struggling to keep up with rapidly increasing datacenter access link bandwidths due to their unsustainable CPU overheads. Motivated by this, our community is exploring a multitude of solutions for future network stacks: from Linux kernel optimizations to partial hardware offload to clean-slate userspace stacks to specialized host network hardware. The design space explored by these solutions would benefit from a detailed understanding of CPU inefficiencies in existing network stacks.This paper presents measurement and insights for Linux kernel network stack performance for 100Gbps access link bandwidths. Our study reveals that such high bandwidth links, coupled with relatively stagnant technology trends for other host resources (e.g., CPU speeds and capacity, cache sizes, NIC buffer sizes, etc.), mark a fundamental shift in host network stack bottlenecks. For instance, we find that a single core is no longer able to process packets at line rate, with data copy from kernel to application buffers at the receiver becoming the core performance bottleneck. In addition, increase in bandwidth-delay products have outpaced the increase in cache sizes, resulting in inefficient DMA pipeline between the NIC and the CPU. Finally, we find that traditional loosely-coupled design of network stack and CPU schedulers in existing operating systems becomes a limiting factor in scaling network stack performance across cores. Based on insights from our study, we discuss implications to design of future operating systems, network protocols, and host hardware.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {65–77},
numpages = {13},
keywords = {datacenter networks, host network stacks, network hardware},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

# Andy file name:
# 2022-wang-et-al-predictable-vfabric-on-informative-data-plane.pdf
@inproceedings{WGQLe2022,
author = {Wang, Shuai and Gao, Kaihui and Qian, Kun and Li, Dan and Miao, Rui and Li, Bo and Zhou, Yu and Zhai, Ennan and Sun, Chen and Gao, Jiaqi and Zhang, Dai and Fu, Binzhang and Kelly, Frank and Cai, Dennis and Liu, Hongqiang Harry and Zhang, Ming},
title = {Predictable vFabric on informative data plane},
year = {2022},
isbn = {9781450394208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544216.3544241},
doi = {10.1145/3544216.3544241},
abstract = {In multi-tenant data centers, each tenant desires reassuring predictability from the virtual network fabric - bandwidth guarantee, work conservation, and bounded tail latency. Achieving these goals simultaneously relies on rapid and precise traffic admission. However, the slow convergence (tens of milliseconds) of prior works can hardly satisfy the increasingly rigorous performance demand under dynamic traffic patterns. Further, state-of-the-art load balance schemes are all guarantee-agnostic and bring great risks on breaking bandwidth guarantee, which is overlooked in prior works.In this paper, we propose μFab, a predictable virtual fabric solution which can (1) explicitly select proper paths for all flows and (2) converge to ideal bandwidth allocation at sub-millisecond timescales. The core idea of μFab is to leverage the programmable data plane to build a fusion of an active edge (e.g., NIC) and an informative core (e.g., switch), where the core sends link status and tenant information to the edge via telemetry to help the latter make a timely and accurate decision on path selection and traffic admission. We fully implement μFab with commodity SmartNICs and programmable switches. Evaluations show that μFab can keep minimum bandwidth guarantee with high bandwidth utilization and near-optimal transmission latency in various network situations with limited probing bandwidth overhead. Application-level experiments, e.g., compute and storage scenarios, show that μFab can improve QPS by 2.5\texttimes{} and cut tail latency by more than 21\texttimes{} compared to the alternatives.},
booktitle = {Proceedings of the ACM SIGCOMM 2022 Conference},
pages = {615–632},
numpages = {18},
keywords = {programmable data plane, performance isolation},
location = {Amsterdam, Netherlands},
series = {SIGCOMM '22}
}

# Andy file name:
# 2022-ghabashneh-et-al-a-microscopic-view-of-bursts-buffer-contention-and-loss-in-data-centers.pdf
@inproceedings{GZLSe2022,
author = {Ghabashneh, Ehab and Zhao, Yimeng and Lumezanu, Cristian and Spring, Neil and Sundaresan, Srikanth and Rao, Sanjay},
title = {A microscopic view of bursts, buffer contention, and loss in data centers},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561430},
doi = {10.1145/3517745.3561430},
abstract = {Managing data center networks with low loss requires understanding traffic dynamics at short (millisecond) time-scales, especially the burstiness of traffic, and to what extent bursts contend for switch buffer resources. Yet, monitoring traffic over such intervals is a challenge at scale.We make two contributions. First, we present Millisampler, a lightweight traffic characterization tool deployed across all Meta hosts. Millisampler takes a host-centric perspective to data collection, which is scalable and allows for correlating traffic patterns with transport layer statistics. Further, simultaneous collection of Millisampler data across servers in a rack enables analysis of how synchronized traffic interacts in rack buffers. In particular, we study contention, which occurs when multiple bursts arrive simultaneously at the dynamically shared rack buffer.Second, we present a data-center-scale analysis of contention, including a unique joint analysis of burstiness, contention, and loss.Our results show (i) contention characteristics vary widely across and within a region and is influenced by service placement; (ii) contention varies significantly over short time-scales; (iii) bursts are likely to encounter some contention; and (iv) higher contention need not lead to more loss, and the interplay with workload and burst properties matters. We discuss implications for data center design including service placement, buffer sharing algorithms and congestion control.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {567–580},
numpages = {14},
location = {Nice, France},
series = {IMC '22}
}

# Andy file name:
# 2022-qureshi-et-al-plb-congestion-signals-are-simple-and-effective-for-network-load-balancing.pdf
@inproceedings{QCYFe2022,
author = {Qureshi, Mubashir Adnan and Cheng, Yuchung and Yin, Qianwen and Fu, Qiaobin and Kumar, Gautam and Moshref, Masoud and Yan, Junhua and Jacobson, Van and Wetherall, David and Kabbani, Abdul},
title = {PLB: congestion signals are simple and effective for network load balancing},
year = {2022},
isbn = {9781450394208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544216.3544226},
doi = {10.1145/3544216.3544226},
abstract = {We present a new, host-based design for link load balancing and report the first experiences of link imbalance in datacenters. Our design, PLB (Protective Load Balancing), builds on transport protocols and ECMP/WCMP to reduce network hotspots. PLB randomly changes the paths of connections that experience congestion, preferring to repath after idle periods to minimize packet reordering. It repaths a connection by changing the IPv6 Flow Label on its packets, which switches include as part of ECMP/WCMP. Across hosts, this action drives down hotspots in the network, and lowers the latency of RPCs.PLB is used fleetwide at Google for TCP and Pony Express traffic. We could deploy it when other designs were infeasible because PLB requires only small transport modifications and switch configuration changes, and is backwards-compatible. It has produced excellent gains: the median utilization imbalance of highly-loaded ToR uplinks in Google datacenters fell by 60\%, packet drops correspondingly fell by 33\%, and the tail latency (99p) of small RPCs fell by 20\%. PLB is also a general solution that works for settings from datacenters to backbone networks, as well as different transports.},
booktitle = {Proceedings of the ACM SIGCOMM 2022 Conference},
pages = {207–218},
numpages = {12},
keywords = {congestion control, datacenter fabric, distributed, load balancing},
location = {Amsterdam, Netherlands},
series = {SIGCOMM '22}
}

# alternate url
# https://web-backend.simula.no/sites/default/files/publications/files/pi2_conext.pdf
# Andy file name:
# 2016-de-schepper-et-al-pi2-a-linearized-aqm-for-both-classic-and-scalable-tcp.pdf
@inproceedings{DBTB2016,
author = {De Schepper, Koen and Bondarenko, Olga and Tsang, Ing-Jyh and Briscoe, Bob},
title = {PI2: A Linearized AQM for both Classic and Scalable TCP},
year = {2016},
isbn = {9781450342926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2999572.2999578},
doi = {10.1145/2999572.2999578},
abstract = {This paper concerns the use of Active Queue Management (AQM) to reduce queuing delay. It offers insight into why it has proved hard for a Proportional Integral (PI) controller to remain both responsive and stable while controlling `Classic' TCP flows, such as TCP Reno and Cubic. Due to their non-linearity, the controller's adjustments have to be smaller when the target drop probability is lower. The PI Enhanced (PIE) algorithm attempts to solve this problem by scaling down the adjustments of the controller using a look-up table. Instead, we control an internal variable that is by definition linearly proportional to the load, then post-process it into the required Classic drop probability---in fact we show that the output simply needs to be squared. This allows tighter control, giving responsiveness and stability better or no worse than PIE achieves, but without all its corrective heuristics.Additionally, with suitable packet classification, it becomes simple to extend this PI2 AQM to support coexistence between Classic and Scalable congestion controls in the public Internet. Unlike a Classic congestion control, a Scalable congestion control ensures sufficient feedback at any flow rate, an example being Data Centre TCP (DCTCP). A Scalable control is linear, so we can use the internal variable directly without any squaring, by omitting the post-processing stage.We implemented this PI2 AQM as a Linux qdisc to extensively test our claims using Classic and Scalable TCPs.},
booktitle = {Proceedings of the 12th International on Conference on Emerging Networking EXperiments and Technologies},
pages = {105–119},
numpages = {15},
keywords = {algorithms, aqm, congestion control, fairness, latency, qos, scalability, scheduling, starvation, tcp, testbed evaluation},
location = {Irvine, California, USA},
series = {CoNEXT '16}
}

# Andy file name:
# 2015-de-schepper-et-al-data-centre-to-the-home-ultra-low-latency-for-all.pdf
@misc{DBTB2015,
  keywords = {Evaluation, congestion control, latency, tcp, Internet, AQM, networks, Data Communication, QoS, Performance, Congestion Avoidance, Algorithms, Design, Analysis, Scaling},
  author = {Koen De Schepper and Olga Bondarenko and Inton Tsang and Bob Briscoe},
  title = {{\textquoteleft}Data Center to the Home{\textquoteright}:  Ultra-Low  Latency for All},
  year = {2015},
  month = {06/2015},
  publisher = {RITE Project},
  url = {http://riteproject.eu/publications/},
}

# Andy file name:
# 2015-briscoe-et-al-scaling-tcps-congestion-window-for-small-round-trip-times.pdf
@article{BD2019,
  author       = {Bob Briscoe and
                  Koen De Schepper},
  title        = {Scaling TCP's Congestion Window for Small Round Trip Times},
  journal      = {CoRR},
  volume       = {abs/1904.07598},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.07598},
  eprinttype    = {arXiv},
  eprint       = {1904.07598},
  timestamp    = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-07598.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

# alternate url
# https://www.academia.edu/10305026/Rate_control_for_communication_networks_shadow_prices_proportional_fairness_and_stability
# Andy file name:
# 1998-kelly-et-al-rate-control-for-communication-networks-shadow-prices-proportional-fairness-and-stbility.pdf
@article{KMT1998,
author = {F. P. Kelly, A. K. Maulloo, and D. K. H. Tan},
title = {Rate control for communication networks: shadow prices, proportional fairness and stability},
journal = {Journal of the Operational Research Society},
volume = {49},
number = {3},
pages = {237--252},
year = {1998},
publisher = {Taylor \& Francis},
doi = {10.1057/palgrave.jors.2600523},
URL = {https://doi.org/10.1057/palgrave.jors.2600523},
eprint = {https://doi.org/10.1057/palgrave.jors.2600523}
}

# alternate url
# https://www.statslab.cam.ac.uk/~frank/elastic.pdf?ref=blog.anoma.net
# Andy file name:
# 1997-kelly-charging-and-rate-control-for-elastic-traffic.pdf
@article{Kell1997,
author = {Kelly, Frank},
title = {Charging and rate control for elastic traffic},
journal = {European Transactions on Telecommunications},
volume = {8},
number = {1},
pages = {33-37},
doi = {https://doi.org/10.1002/ett.4460080106},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.4460080106},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ett.4460080106},
abstract = {Abstract This paper addresses the issues of charging, rate control and routing for a communication network carrying elastic traffic, such as an ATM network offering an available bit rate service. A model is described from which max-min fairness of rates emerges as a limiting special case; more generally, the charges users are prepared to pay influence their allocated rates. In the preferred version of the model, a user chooses the charge per unit time that the user will pay; thereafter the user's rate is determined by the network according to a proportional fairness criterion applied to the rate per unit charge. A system optimum is achieved when users' choices of charges and the network's choice of allocated rates are in equilibrium.},
year = {1997}
}

# slides from a talk on this topic:
# https://www.bobbriscoe.net/presents/0611ietf/0611briscoe-fair-tsvarea.pdf
# alternate url
# https://www.researchgate.net/publication/220194938_Flow_Rate_Fairness_Dismantling_a_Religion
# Andy file name:
# 2007-briscoe-flow-rate-fairness-dismantling-a-religion.pdf 
@article{Bris2007,
author = {Briscoe, Bob},
title = {Flow rate fairness: dismantling a religion},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/1232919.1232926},
doi = {10.1145/1232919.1232926},
abstract = {Resource allocation and accountability keep reappearing on every list of requirements for the Internet architecture. The reason we never resolve these issues is a broken idea of what the problem is. The applied research and standards communities are using completely unrealistic and impractical fairness criteria. The resulting mechanisms don't even allocate the right thing and they don't allocate it between the right entities. We explain as bluntly as we can that thinking about fairness mechanisms like TCP in terms of sharing out flow rates has no intellectual heritage from any concept of fairness in philosophy or social science, or indeed real life. Comparing flow rates should never again be used for claims of fairness in production networks. Instead, we should judge fairness mechanisms on how they share out the 'cost' of each user's actions on others},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {mar},
pages = {63–74},
numpages = {12},
keywords = {resource allocation, identity, fairness, congestion control, accountability}
}

# alternate url
# https://people.eecs.berkeley.edu/~istoica/classes/cs268/10/papers/afd.pdf
# Andy file name:
# 2003-pan-et-al-approximate-fairness-through-differential-dropping.pdf
@article{PBPS2003,
author = {Pan, Rong and Breslau, Lee and Prabhakar, Balaji and Shenker, Scott},
title = {Approximate fairness through differential dropping},
year = {2003},
issue_date = {April 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/956981.956985},
doi = {10.1145/956981.956985},
abstract = {Many researchers have argued that the Internet architecture would be more robust and more accommodating of heterogeneity if routers allocated bandwidth fairly. However, most of the mechanisms proposed to accomplish this, such as Fair Queueing [16, 6] and its many variants [2, 23, 15], involve complicated packet scheduling algorithms. These algorithms, while increasingly common in router designs, may not be inexpensively implementable at extremely high speeds; thus, finding more easily implementable variants of such algorithms may be of significant practical value. This paper proposes an algorithm called Approximate Fair Dropping (AFD), which bases its dropping decisions on the recent history of packet arrivals. AFD retains a simple forwarding path and requires an amount of additional state that is small compared to current packet buffers. Simulation results, which we describe here, suggest that the design provides a reasonable degree of fairness in a wide variety of operating conditions. The performance of our approach is aided by the fact that the vast majority of Internet flows are slow but the fast flows send the bulk of the bits. This allows a small sample of recent history to provide accurate rate estimates of the fast flows.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {apr},
pages = {23–39},
numpages = {17}
}

# alternate url
# https://www.cs.purdue.edu/homes/fahmy/papers/2020conext.pdf
# Andy file name:
# 2020-taheri-et-al-rocc-robust-congestion-control-for-rdma.pdf
@inproceedings{TMVFe2020,
author = {Taheri, Parvin and Menikkumbura, Danushka and Vanini, Erico and Fahmy, Sonia and Eugster, Patrick and Edsall, Tom},
title = {RoCC: robust congestion control for RDMA},
year = {2020},
isbn = {9781450379489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386367.3431316},
doi = {10.1145/3386367.3431316},
abstract = {In this paper, we present RoCC, a robust congestion control approach for datacenter networks based on RDMA. RoCC leverages switch queue size as an input to a PI controller, which computes the fair data rate of flows in the queue, signaling it to the flow sources. The PI parameters are self-tuning to guarantee stability, rapid convergence, and fair and near-optimal throughput in a wide range of congestion scenarios. Our simulation and DPDK implementation results show that RoCC can achieve up to 7\texttimes{} reduction in PFC frames generated under high average load levels, compared to DCQCN. At the same time, RoCC can achieve up to 8\texttimes{} lower tail latency, compared to DCQCN and HPCC. We also find that RoCC does not require PFC. The functional components of RoCC are implementable in P4-based and fixed-function switch ASICs.},
booktitle = {Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
pages = {17–30},
numpages = {14},
keywords = {network programmability, datacenter, congestion control, RDMA},
location = {Barcelona, Spain},
series = {CoNEXT '20}
}

# Andy file name:
# 2017-shpiner-et-al-roce-rocks-without-pfc-detailed-evaluation.pdf
@inproceedings{SZDBe2017,
author = {Shpiner, Alexander and Zahavi, Eitan and Dahley, Omar and Barnea, Aviv and Damsker, Rotem and Yekelis, Gennady and Zus, Michael and Kuta, Eitan and Baram, Dean},
title = {RoCE Rocks without PFC: Detailed Evaluation},
year = {2017},
isbn = {9781450350532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098583.3098588},
doi = {10.1145/3098583.3098588},
abstract = {In recent years, the usage of RDMA in data center networks has increased significantly, with RDMA over Converged Ethernet (RoCE) emerging as the canonical approach for deploying RDMA in Ethernet-based data centers. Initial implementations of RoCE required a lossless fabric for optimal performance. This is typically achieved by enabling Priority Flow Control (PFC) on Ethernet NICs and switches. The RoCEv2 specification introduced RoCE congestion control, which allows throttling the transmission rate in response to congestion. Consequently, packet loss is minimized and performance is maintained, even if the underlying Ethernet network is lossy.In this paper, we discuss the latest developments in RoCE congestion control. Hardware congestion control reduces the latency of the congestion control loop; it reacts promptly in the face of congestion by throttling the transmission rate quickly and accurately. The short control loop also prevents network buffers from overfilling under various congestion scenarios. In addition, fast hardware retransmission complements congestion control in severe congestion scenarios, by significantly reducing the performance penalty of packet drops. We survey architectural features that allow deployment of RoCE over lossy networks and present real lab test results.},
booktitle = {Proceedings of the Workshop on Kernel-Bypass Networks},
pages = {25–30},
numpages = {6},
keywords = {Congestion Control, Performance, RoCE},
location = {Los Angeles, CA, USA},
series = {KBNets '17}
}
