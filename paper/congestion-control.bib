

# Andy file name:
# 1994-brakmo-et-al-tcp-vegas-new-techniques-for-congestion-detection-and-avoidance.pdf
@inproceedings{BOP1994,
author = {Brakmo, Lawrence S. and O'Malley, Sean W. and Peterson, Larry L.},
title = {TCP Vegas: new techniques for congestion detection and avoidance},
year = {1994},
isbn = {0897916824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/190314.190317},
doi = {10.1145/190314.190317},
abstract = {Vegas is a new implementation of TCP that achieves between 40 and 70\% better throughput, with one-fifth to one-half the losses, as compared to the implementation of TCP in the Reno distribution of BSD Unix. This paper motivates and describes the three key techniques employed by Vegas, and presents the results of a comprehensive experimental performance study—using both simulations and measurements on the Internet—of the Vegas and Reno implementations of TCP.},
booktitle = {Proceedings of the Conference on Communications Architectures, Protocols and Applications},
pages = {24–35},
numpages = {12},
location = {London, United Kingdom},
series = {SIGCOMM '94}
}

# alternate url:
# https://cseweb.ucsd.edu/classes/wi01/cse222/papers/brakmo-vegas-jsac95.pdf
# Andy file name:
# 1995-brakmo-et-al-tcp-vegas-end-to-end-congestion-avoidance-on-a-global-internet.pdf
@article{BP1995,
  author={Brakmo, L.S. and Peterson, L.L.},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={TCP Vegas: end to end congestion avoidance on a global Internet}, 
  year={1995},
  volume={13},
  number={8},
  pages={1465-1480},
  keywords={Internet;Protocols;Throughput;Testing;Bandwidth;Programmable control;Adaptive control;Jacobian matrices;Computer science;TCPIP},
  doi={10.1109/49.464716}
}

# Andy file name:
# 1996-hoe-improving-the-start-up-behavior-of-a-congestion-control-scheme-for-tcp.pdf
@inproceedings{Hoe1996,
author = {Hoe, Janey C.},
title = {Improving the start-up behavior of a congestion control scheme for TCP},
year = {1996},
isbn = {0897917901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/248156.248180},
doi = {10.1145/248156.248180},
abstract = {Based on experiments conducted in a network simulator and over real networks, this paper proposes changes to the congestion control scheme in current TCP implementations to improve its behavior during the start-up period of a TCP connection.The scheme, which includes Slow-start, Fast Retransmit, and Fast Recovery algorithms, uses acknowledgments from a receiver to dynamically calculate reasonable operating values for a sender's TCP parameters governing when and how much a sender can pump into the network. During the start-up period, because a TCP sender starts with default parameters, it often ends up sending too many packets and too fast, leading to multiple losses of packets from the same window. This paper shows that recovery from losses during this start-up period is often unnecessarily time-consuming.In particular, using the current Fast Retransmit algorithm, when multiple packets in the same window are lost, only one of the packet losses may be recovered by each Fast Retransmit; the rest are often recovered by Slow-start after a usually lengthy retransmission timeout. Thus, this paper proposes changes to the Fast Retransmit algorithm so that it can quickly recover from multiple packet losses without waiting unnecessarily for the timeout. These changes, tested in the simulator and on the real networks, show significant performance improvements, especially for short TCP transfers. The paper also proposes other changes to help minimize the number of packets lost during the start-up period.},
booktitle = {Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications},
pages = {270–280},
numpages = {11},
location = {Palo Alto, California, USA},
series = {SIGCOMM '96}
}

# alternate url:
# https://www.cs.princeton.edu/courses/archive/fall16/cos561/papers/Cubic08.pdf
# Andy file name:
# 2008-ha-et-al-cubic-a-new-tcp-friendly-high-speed-tcp-variant.pdf
@article{HRX2008,
author = {Ha, Sangtae and Rhee, Injong and Xu, Lisong},
title = {CUBIC: a new TCP-friendly high-speed TCP variant},
year = {2008},
issue_date = {July 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/1400097.1400105},
doi = {10.1145/1400097.1400105},
abstract = {CUBIC is a congestion control protocol for TCP (transmission control protocol) and the current default TCP algorithm in Linux. The protocol modifies the linear window growth function of existing TCP standards to be a cubic function in order to improve the scalability of TCP over fast and long distance networks. It also achieves more equitable bandwidth allocations among flows with different RTTs (round trip times) by making the window growth to be independent of RTT -- thus those flows grow their congestion window at the same rate. During steady state, CUBIC increases the window size aggressively when the window is far from the saturation point, and the slowly when it is close to the saturation point. This feature allows CUBIC to be very scalable when the bandwidth and delay product of the network is large, and at the same time, be highly stable and also fair to standard TCP flows. The implementation of CUBIC in Linux has gone through several upgrades. This paper documents its design, implementation, performance and evolution as the default TCP algorithm of Linux.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jul},
pages = {64–74},
numpages = {11}
}

# Andy file name:
# 2010-alizadeh-et-al-data-center-tcp-dctcp.pdf
@inproceedings{AGMPe2010,
author = {Alizadeh, Mohammad and Greenberg, Albert and Maltz, David A. and Padhye, Jitendra and Patel, Parveen and Prabhakar, Balaji and Sengupta, Sudipta and Sridharan, Murari},
title = {Data center TCP (DCTCP)},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851192},
doi = {10.1145/1851182.1851192},
abstract = {Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry "background" flows build up queues at the switches, and thus impact the performance of latency sensitive "foreground" traffic.To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90\% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {63–74},
numpages = {12},
keywords = {data center network, TCP, ECN},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

# alternate url:
# https://conferences.sigcomm.org/sigcomm/2011/papers/sigcomm/p50.pdf
# Andy file name:
# 2011-wilson-et-al-better-never-than-late-meeting-deadlines-in-datacenter-networks.pdf
@inproceedings{WBKR2011,
author = {Wilson, Christo and Ballani, Hitesh and Karagiannis, Thomas and Rowtron, Ant},
title = {Better never than late: meeting deadlines in datacenter networks},
year = {2011},
isbn = {9781450307970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2018436.2018443},
doi = {10.1145/2018436.2018443},
abstract = {The soft real-time nature of large scale web applications in today's datacenters, combined with their distributed workflow, leads to deadlines being associated with the datacenter application traffic. A network flow is useful, and contributes to application throughput and operator revenue if, and only if, it completes within its deadline. Today's transport pro- tocols (TCP included), given their Internet origins, are agnostic to such flow deadlines. Instead, they strive to share network resources fairly. We show that this can hurt application performance.Motivated by these observations, and other (previously known) deficiencies of TCP in the datacenter environment, this paper presents the design and implementation of D3, a deadline-aware control protocol that is customized for the datacenter environment. D3 uses explicit rate control to apportion bandwidth according to flow deadlines. Evaluation from a 19-node, two-tier datacenter testbed shows that D3, even without any deadline information, easily outper- forms TCP in terms of short flow latency and burst tolerance. Further, by utilizing deadline information, D3 effectively doubles the peak load that the datacenter network cansupport.},
booktitle = {Proceedings of the ACM SIGCOMM 2011 Conference},
pages = {50–61},
numpages = {12},
keywords = {sla, rate control, online services, deadline, datacenter},
location = {Toronto, Ontario, Canada},
series = {SIGCOMM '11}
}

# alternate url:
# https://www.usenix.org/legacy/events/nsdi11/tech/full_papers/Wischik.pdf
# Andy file name:
# todo
@inproceedings{WRGH2011,
author = {Wischik, Damon and Raiciu, Costin and Greenhalgh, Adam and Handley, Mark},
title = {Design, implementation and evaluation of congestion control for multipath TCP},
year = {2011},
publisher = {USENIX Association},
address = {USA},
abstract = {Multipath TCP, as proposed by the IETF working group mptcp, allows a single data stream to be split across multiple paths. This has obvious benefits for reliability, and it can also lead to more efficient use of networked resources. We describe the design of a multipath congestion control algorithm, we implement it in Linux, and we evaluate it for multihomed servers, data centers and mobile clients. We show that some 'obvious' solutions for multipath congestion control can be harmful, but that our algorithm improves throughput and fairness compared to single-path TCP. Our algorithmis a drop-in replacement for TCP, and we believe it is safe to deploy.},
booktitle = {Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation},
pages = {99–112},
numpages = {14},
location = {Boston, MA},
series = {NSDI'11}
}

# alternate url:
# https://web.stanford.edu/~balaji/papers/11analysisof.pdf
# Andy file name:
# 2011-alizadeh-et-al-analysis-of-dctcp-stability-convergence-and-fairness.pdf
@inproceedings{AJP2011,
author = {Alizadeh, Mohammad and Javanmard, Adel and Prabhakar, Balaji},
title = {Analysis of DCTCP: stability, convergence, and fairness},
year = {2011},
isbn = {9781450308144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993744.1993753},
doi = {10.1145/1993744.1993753},
abstract = {Cloud computing, social networking and information networks (for search, news feeds, etc) are driving interest in the deployment of large data centers. TCP is the dominant Layer 3 transport protocol in these networks. However, the operating conditions---very high bandwidth links, low round-trip times, small-buffered switches---and traffic patterns cause TCP to perform very poorly. The Data Center TCP (DCTCP) algorithm has recently been proposed as a TCP variant for data centers and addresses these shortcomings.In this paper, we provide a mathematical analysis of DCTCP. We develop a fluid model of DCTCP and use it to analyze the throughput and delay performance of the algorithm, as a function of the design parameters and of network conditions like link speeds, round-trip times and the number of active flows. Unlike fluid model representations of standard congestion control loops, the DCTCP fluid model exhibits limit cycle behavior. Therefore, it is not amenable to analysis by linearization around a fixed point and we undertake a direct analysis of the limit cycles, proving their stability. Using a hybrid (continuous- and discrete-time) model, we analyze the convergence of DCTCP sources to their fair share, obtaining an explicit characterization of the convergence rate. Finally, we investigate the "RTT-fairness" of DCTCP; i.e., the rate obtained by DCTCP sources as a function of their RTTs. We find a very simple change to DCTCP which is suggested by the fluid model and which significantly improves DCTCP's RTT-fairness. We corroborate our results with ns2 simulations.},
booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {73–84},
numpages = {12},
keywords = {data center network, congestion control, analysis, TCP},
location = {San Jose, California, USA},
series = {SIGMETRICS '11}
}

# alternate url:
# https://people.csail.mit.edu/alizadeh/papers/afqcn-hoti10.pdf
# Andy file name:
# 2010-kabbani-et-al-af-qcn-approximate-fairness-with-quantized-congestion-notification-for-multi-tenanted-data-centers.pdf
@inproceedings{KAYPP2010,
author = {Kabbani, Abdul and Alizadeh, Mohammad and Yasuda, Masato and Pan, Rong and Prabhakar, Balaji},
title = {AF-QCN: Approximate Fairness with Quantized Congestion Notification for Multi-tenanted Data Centers},
year = {2010},
isbn = {9780769542089},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HOTI.2010.26},
doi = {10.1109/HOTI.2010.26},
abstract = {Data Center Networks represent the convergence of computing and networking, of data and storage networks, and of packet transport mechanisms in Layers 2 and 3. Congestion control algorithms are a key component of data transport in this type of network. Recently, a Layer 2 congestion management algorithm, called QCN (Quantized Congestion Notification), has been adopted for the IEEE 802.1 Data Center Bridging standard: IEEE 802.1Qau. The QCN algorithm has been designed to be stable, responsive, and simple to implement. However, it does not provide weighted fairness, where the weights can be set by the operator on a per-flow or per-class basis. Such a feature can be very useful in multi-tenanted Cloud Computing and Data Center environments. This paper addresses this issue. Specifically, we develop an algorithm, called AF-QCN (for Approximately Fair QCN), which ensures a faster convergence to fairness than QCN, maintains this fairness at fine-grained time scales, and provides programmable weighted fair bandwidth shares to flows/flow-classes. It combines the QCN algorithm developed by some of the authors of this paper, and the AFD algorithm previously developed by Pan et. al. AF-QCN requires no modifications to a QCN source (Reaction Point) and introduces a very light-weight addition to a QCNcapable switch (Congestion Point). The results obtained through simulations and an FPGA implementation on a 1Gbps platform show that AF-QCN retains the good congestion management performance of QCN while achieving rapid and programmable (approximate) weighted fairness.},
booktitle = {Proceedings of the 2010 18th IEEE Symposium on High Performance Interconnects},
pages = {58–65},
numpages = {8},
series = {HOTI '10}
}

# alternate url
# https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final187.pdf
# Andy file name:
# 2012-alizadeh-et-al-less-is-more-trading-a-little-bandwidth-for-ultra-low-latency-in-the-data-center.pdf
@inproceedings{AKEPe2012,
author = {Alizadeh, Mohammad and Kabbani, Abdul and Edsall, Tom and Prabhakar, Balaji and Vahdat, Amin and Yasuda, Masato},
title = {Less is more: trading a little bandwidth for ultra-low latency in the data center},
year = {2012},
publisher = {USENIX Association},
address = {USA},
abstract = {Traditional measures of network goodness--goodput, quality of service, fairness--are expressed in terms of bandwidth. Network latency has rarely been a primary concern because delivering the highest level of bandwidth essentially entails driving up latency--at the mean and, especially, at the tail. Recently, however, there has been renewed interest in latency as a primary metric for mainstream applications. In this paper, we present the HULL (High-bandwidth Ultra-Low Latency) architecture to balance two seemingly contradictory goals: near baseline fabric latency and high bandwidth utilization. HULL leaves 'bandwidth headroom' using Phantom Queues that deliver congestion signals before network links are fully utilized and queues form at switches. By capping utilization at less than link capacity, we leave room for latency sensitive traffic to avoid buffering and the associated large delays. At the same time, we use DCTCP, a recently proposed congestion control algorithm, to adaptively respond to congestion and to mitigate the bandwidth penalties which arise from operating in a bufferless fashion. HULL further employs packet pacing to counter burstiness caused by Interrupt Coalescing and Large Send Offloading. Our implementation and simulation results show that by sacrificing a small amount (e.g., 10\%) of bandwidth, HULL can dramatically reduce average and tail latencies in the data center.},
booktitle = {Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation},
pages = {19},
numpages = {1},
location = {San Jose, CA},
series = {NSDI'12}
}

# Andy file name:
# 2012-hong-et-al-finishing-flows-quickly-with-preemptive-scheduling.pdf
@inproceedings{HCG2012,
author = {Hong, Chi-Yao and Caesar, Matthew and Godfrey, P. Brighten},
title = {Finishing flows quickly with preemptive scheduling},
year = {2012},
isbn = {9781450314190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342356.2342389},
doi = {10.1145/2342356.2342389},
abstract = {Today's data centers face extreme challenges in providing low latency. However, fair sharing, a principle commonly adopted in current congestion control protocols, is far from optimal for satisfying latency requirements.We propose Preemptive Distributed Quick (PDQ) flow scheduling, a protocol designed to complete flows quickly and meet flow deadlines. PDQ enables flow preemption to approximate a range of scheduling disciplines. For example, PDQ can emulate a shortest job first algorithm to give priority to the short flows by pausing the contending flows. PDQ borrows ideas from centralized scheduling disciplines and implements them in a fully distributed manner, making it scalable to today's data centers. Further, we develop a multipath version of PDQ to exploit path diversity.Through extensive packet-level and flow-level simulation, we demonstrate that PDQ significantly outperforms TCP, RCP and D3 in data center environments. We further show that PDQ is stable, resilient to packet loss, and preserves nearly all its performance gains even given inaccurate flow information.},
booktitle = {Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {127–138},
numpages = {12},
keywords = {data center, deadline, flow scheduling},
location = {Helsinki, Finland},
series = {SIGCOMM '12}
}

# Andy file name:
# 2012-vamanan-et-al-deadline-aware-datacenter-tc-d2tcp.pdf
@inproceedings{VHV2012,
author = {Vamanan, Balajee and Hasan, Jahangir and Vijaykumar, T.N.},
title = {Deadline-aware datacenter tcp (D2TCP)},
year = {2012},
isbn = {9781450314190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342356.2342388},
doi = {10.1145/2342356.2342388},
abstract = {An important class of datacenter applications, called Online Data-Intensive (OLDI) applications, includes Web search, online retail, and advertisement. To achieve good user experience, OLDI applications operate under soft-real-time constraints (e.g., 300 ms latency) which imply deadlines for network communication within the applications. Further, OLDI applications typically employ tree-based algorithms which, in the common case, result in bursts of children-to-parent traffic with tight deadlines. Recent work on datacenter network protocols is either deadline-agnostic (DCTCP) or is deadline-aware (D3) but suffers under bursts due to race conditions. Further, D3 has the practical drawbacks of requiring changes to the switch hardware and not being able to coexist with legacy TCP. We propose Deadline-Aware Datacenter TCP (D2TCP), a novel transport protocol, which handles bursts, is deadline-aware, and is readily deployable. In designing D2TCP, we make two contributions: (1) D2TCP uses a distributed and reactive approach for bandwidth allocation which fundamentally enables D2TCP's properties. (2) D2TCP employs a novel congestion avoidance algorithm, which uses ECN feedback and deadlines to modulate the congestion window via a gamma-correction function. Using a small-scale implementation and at-scale simulations, we show that D2TCP reduces the fraction of missed deadlines compared to DCTCP and D3 by 75\% and 50\%, respectively.},
booktitle = {Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {115–126},
numpages = {12},
keywords = {cloud services, datacenter, deadline, ecn, oldi, sla, tcp},
location = {Helsinki, Finland},
series = {SIGCOMM '12}
}

# alternate url
# https://web.stanford.edu/~skatti/pubs/sigcomm13-pfabric.pdf
# Andy file name:
# 2013-alizadeh-et-al-pfabric-minimal-near-optimal-datacenter-transport.pdf
@inproceedings{AYSKe2013,
author = {Alizadeh, Mohammad and Yang, Shuang and Sharif, Milad and Katti, Sachin and McKeown, Nick and Prabhakar, Balaji and Shenker, Scott},
title = {pFabric: minimal near-optimal datacenter transport},
year = {2013},
isbn = {9781450320566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486001.2486031},
doi = {10.1145/2486001.2486031},
abstract = {In this paper we present pFabric, a minimalistic datacenter transport design that provides near theoretically optimal flow completion times even at the 99th percentile for short flows, while still minimizing average flow completion time for long flows. Moreover, pFabric delivers this performance with a very simple design that is based on a key conceptual insight: datacenter transport should decouple flow scheduling from rate control. For flow scheduling, packets carry a single priority number set independently by each flow; switches have very small buffers and implement a very simple priority-based scheduling/dropping mechanism. Rate control is also correspondingly simpler; flows start at line rate and throttle back only under high and persistent packet loss. We provide theoretical intuition and show via extensive simulations that the combination of these two simple mechanisms is sufficient to provide near-optimal performance.},
booktitle = {Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM},
pages = {435–446},
numpages = {12},
keywords = {datacenter network, flow scheduling, packet transport},
location = {Hong Kong, China},
series = {SIGCOMM '13}
}

# alternate url
# https://2022-cs244.github.io/papers/L15-TCP-ex-machina.pdf
# Andy file name:
# 2013-winstein-et-al-tcp-ex-machina-computer-generated-congestion-control.pdf
@inproceedings{WB2013,
author = {Winstein, Keith and Balakrishnan, Hari},
title = {TCP ex machina: computer-generated congestion control},
year = {2013},
isbn = {9781450320566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486001.2486020},
doi = {10.1145/2486001.2486020},
abstract = {This paper describes a new approach to end-to-end congestion control on a multi-user network. Rather than manually formulate each endpoint's reaction to congestion signals, as in traditional protocols, we developed a program called Remy that generates congestion-control algorithms to run at the endpoints.In this approach, the protocol designer specifies their prior knowledge or assumptions about the network and an objective that the algorithm will try to achieve, e.g., high throughput and low queueing delay. Remy then produces a distributed algorithm---the control rules for the independent endpoints---that tries to achieve this objective.In simulations with ns-2, Remy-generated algorithms outperformed human-designed end-to-end techniques, including TCP Cubic, Compound, and Vegas. In many cases, Remy's algorithms also outperformed methods that require intrusive in-network changes, including XCP and Cubic-over-sfqCoDel (stochastic fair queueing with CoDel for active queue management). Remy can generate algorithms both for networks where some parameters are known tightly a priori, e.g. datacenters, and for networks where prior knowledge is less precise, such as cellular networks. We characterize the sensitivity of the resulting performance to the specificity of the prior knowledge, and the consequences when real-world conditions contradict the assumptions supplied at design-time.},
booktitle = {Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM},
pages = {123–134},
numpages = {12},
keywords = {computer-designed algorithms, congestion control},
location = {Hong Kong, China},
series = {SIGCOMM '13}
}

# Andy file name:
# 2013-winstein-et-al-stochastic-forecasts-achieve-high-throughput-and-low-delay-over-cellular-networks.pdf
@inproceedings{WSB2013,
author = {Winstein, Keith and Sivaraman, Anirudh and Balakrishnan, Hari},
title = {Stochastic forecasts achieve high throughput and low delay over cellular networks},
year = {2013},
publisher = {USENIX Association},
address = {USA},
abstract = {Sprout is an end-to-end transport protocol for interactive applications that desire high throughput and low delay. Sprout works well over cellular wireless networks, where link speeds change dramatically with time, and current protocols build up multi-second queues in network gateways. Sprout does not use TCP-style reactive congestion control; instead the receiver observes the packet arrival times to infer the uncertain dynamics of the network path. This inference is used to forecast how many bytes may be sent by the sender, while bounding the risk that packets will be delayed inside the network for too long.In evaluations on traces from four commercial LTE and 3G networks, Sprout, compared with Skype, reduced self-inflicted end-to-end delay by a factor of 7.9 and achieved 2.2\texttimes{} the transmitted bit rate on average. Compared with Google's Hangout, Sprout reduced delay by a factor of 7.2 while achieving 4.4\texttimes{} the bit rate, and compared with Apple's Facetime, Sprout reduced delay by a factor of 8.7 with 1.9\texttimes{} the bit rate.Although it is end-to-end, Sprout matched or outperformed TCP Cubic running over the CoDel active queue management algorithm, which requires changes to cellular carrier equipment to deploy. We also tested Sprout as a tunnel to carry competing interactive and bulk traffic (Skype and TCP Cubic), and found that Sprout was able to isolate client application flows from one another.},
booktitle = {Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation},
pages = {459–472},
numpages = {14},
location = {Lombard, IL},
series = {nsdi'13}
}

# alternate url
# https://people.csail.mit.edu/alizadeh/papers/conga-sigcomm14.pdf
# Andy file name:
# 2014-alizadeh-et-al-conga-distributed-congestion-aware-load-balancing-for-datacenters.pdf
@inproceedings{AEDVe2014,
author = {Alizadeh, Mohammad and Edsall, Tom and Dharmapurikar, Sarang and Vaidyanathan, Ramanan and Chu, Kevin and Fingerhut, Andy and Lam, Vinh The and Matus, Francis and Pan, Rong and Yadav, Navindra and Varghese, George},
title = {CONGA: distributed congestion-aware load balancing for datacenters},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2626316},
doi = {10.1145/2619239.2626316},
abstract = {We present the design, implementation, and evaluation of CONGA, a network-based distributed congestion-aware load balancing mechanism for datacenters. CONGA exploits recent trends including the use of regular Clos topologies and overlays for network virtualization. It splits TCP flows into flowlets, estimates real-time congestion on fabric paths, and allocates flowlets to paths based on feedback from remote switches. This enables CONGA to efficiently balance load and seamlessly handle asymmetry, without requiring any TCP modifications. CONGA has been implemented in custom ASICs as part of a new datacenter fabric. In testbed experiments, CONGA has 5x better flow completion times than ECMP even with a single link failure and achieves 2-8x better throughput than MPTCP in Incast scenarios. Further, the Price of Anarchy for CONGA is provably small in Leaf-Spine topologies; hence CONGA is nearly as effective as a centralized scheduler while being able to react to congestion in microseconds. Our main thesis is that datacenter fabric load balancing is best done in the network, and requires global schemes such as CONGA to handle asymmetry.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {503–514},
numpages = {12},
keywords = {datacenter fabric, distributed, load balancing},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

# alternate url
# http://fastpass.mit.edu/Fastpass-SIGCOMM14-Perry.pdf
# Andy file name:
# 2014-perry-et-al-fastpass-a-centralized-zero-queue-datacenter-network.pdf
@inproceedings{POBSF2014,
author = {Perry, Jonathan and Ousterhout, Amy and Balakrishnan, Hari and Shah, Devavrat and Fugal, Hans},
title = {Fastpass: a centralized "zero-queue" datacenter network},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2626309},
doi = {10.1145/2619239.2626309},
abstract = {An ideal datacenter network should provide several properties, including low median and tail latency, high utilization (throughput), fair allocation of network resources between users or applications, deadline-aware scheduling, and congestion (loss) avoidance. Current datacenter networks inherit the principles that went into the design of the Internet, where packet transmission and path selection decisions are distributed among the endpoints and routers. Instead, we propose that each sender should delegate control---to a centralized arbiter---of when each packet should be transmitted and what path it should follow.This paper describes Fastpass, a datacenter network architecture built using this principle. Fastpass incorporates two fast algorithms: the first determines the time at which each packet should be transmitted, while the second determines the path to use for that packet. In addition, Fastpass uses an efficient protocol between the endpoints and the arbiter and an arbiter replication strategy for fault-tolerant failover. We deployed and evaluated Fastpass in a portion of Facebook's datacenter network. Our results show that Fastpass achieves high throughput comparable to current networks at a 240x reduction is queue lengths (4.35 Mbytes reducing to 18 Kbytes), achieves much fairer and consistent flow throughputs than the baseline TCP (5200x reduction in the standard deviation of per-flow throughput with five concurrent connections), scalability from 1 to 8 cores in the arbiter implementation with the ability to schedule 2.21 Terabits/s of traffic in software on eight cores, and a 2.5x reduction in the number of TCP retransmissions in a latency-sensitive service at Facebook.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {307–318},
numpages = {12},
keywords = {zero-queue, scheduling, low latency, high throughput, datacenter, data plane, centralized, arbiter},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

# Andy file name:
# 2014-kabbani-et-al-flowbender-flow-level-adaptive-routing-for-improved-latency-and-throughput-in-datacenter-networks.pdf
@inproceedings{KVHD2014,
author = {Kabbani, Abdul and Vamanan, Balajee and Hasan, Jahangir and Duchene, Fabien},
title = {FlowBender: Flow-level Adaptive Routing for Improved Latency and Throughput in Datacenter Networks},
year = {2014},
isbn = {9781450332798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2674005.2674985},
doi = {10.1145/2674005.2674985},
abstract = {Datacenter networks provide high path diversity for traffic between machines. Load balancing traffic across these paths is important for both, latency- and throughput-sensitive applications. The standard load balancing techniques used today obliviously hash a flow to a random path. When long flows collide on the same path, this might lead to long lasting congestion while other paths could be underutilized, degrading performance of other flows as well. Recent proposals to address this shortcoming incur significant implementation complexity at the host that would actually slow down short flows (MPTCP), depend on relatively slow centralized controllers for rerouting large congesting flows (Hedera), or require custom switch hardware, hindering near-term deployment (DeTail).We propose FlowBender, a novel technique that: (1) Load balances distributively at the granularity of flows instead of packets, avoiding excessive packet reordering. (2) Uses end-host-driven rehashing to trigger dynamic flow-to-path assignment. (3) Recovers from link failures within a Retransmit Timeout (RTO). (4) Amounts to less than 50 lines of critical kernel code and is readily deployable in commodity data centers today. (5) Is very robust and simple to tune. We evaluate FlowBender using both simulations and a real testbed implementation, and show that it improves average and tail latencies significantly compared to state of the art techniques without incurring the significant overhead and complexity of other load balancing schemes.},
booktitle = {Proceedings of the 10th ACM International on Conference on Emerging Networking Experiments and Technologies},
pages = {149–160},
numpages = {12},
keywords = {tcp, load balancing, ecmp, data centers},
location = {Sydney, Australia},
series = {CoNEXT '14}
}

# alternate url
# https://conferences2.sigcomm.org/co-next/2015/img/papers/conext15-final1.pdf
# Andy file name:
# 2015-gao-et-al-phost-distributed-near-optimal-datacenter-transport-over-commodity-network-fabric.pdf
@inproceedings{GNKAe2015,
author = {Gao, Peter X. and Narayan, Akshay and Kumar, Gautam and Agarwal, Rachit and Ratnasamy, Sylvia and Shenker, Scott},
title = {pHost: distributed near-optimal datacenter transport over commodity network fabric},
year = {2015},
isbn = {9781450334129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2716281.2836086},
doi = {10.1145/2716281.2836086},
abstract = {The importance of minimizing flow completion times (FCT) in datacenters has led to a growing literature on new network transport designs. Of particular note is pFabric, a protocol that achieves near-optimal FCTs. However, pFabric's performance comes at the cost of generality, since pFabric requires specialized hardware that embeds a specific scheduling policy within the network fabric, making it hard to meet diverse policy goals. Aiming for generality, the recent Fastpass proposal returns to a design based on commodity network hardware and instead relies on a centralized scheduler. Fastpass achieves generality, but (as we show) loses many of pFabric's performance benefits.We present pHost, a new transport design aimed at achieving both: the near-optimal performance of pFabric and the commodity network design of Fastpass. Similar to Fastpass, pHost keeps the network simple by decoupling the network fabric from scheduling decisions. However, pHost introduces a new distributed protocol that allows end-hosts to directly make scheduling decisions, thus avoiding the overheads of Fastpass's centralized scheduler architecture. We show that pHost achieves performance on par with pFabric (within 4\% for typical conditions) and significantly outperforms Fastpass (by a factor of 3.8\texttimes{}) while relying only on commodity network hardware.},
booktitle = {Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies},
articleno = {1},
numpages = {12},
keywords = {packet transport, flow scheduling, datacenter network},
location = {Heidelberg, Germany},
series = {CoNEXT '15}
}

# Andy file name:
# 2015-bai-et-al-information-agnostic-flow-scheduling-for-commodity-data-centers.pdf
@inproceedings{BCCHe2015,
author = {Bai, Wei and Chen, Li and Chen, Kai and Han, Dongsu and Tian, Chen and Wang, Hao},
title = {Information-agnostic flow scheduling for commodity data centers},
year = {2015},
isbn = {9781931971218},
publisher = {USENIX Association},
address = {USA},
abstract = {Many existing data center network (DCN) flow scheduling schemes minimize flow completion times (FCT) based on prior knowledge of flows and custom switch functions, making them superior in performance but hard to use in practice. By contrast, we seek to minimize FCT with no prior knowledge and existing commodity switch hardware.To this end, we present PIAS, a DCN flow scheduling mechanism that aims to minimize FCT by mimicking Shortest Job First (SJF) on the premise that flow size is not known a priori. At its heart, PIAS leverages multiple priority queues available in existing commodity switches to implement a Multiple Level Feedback Queue (MLFQ), in which a PIAS flow is gradually demoted from higher-priority queues to lower-priority queues based on the number of bytes it has sent. As a result, short flows are likely to be finished in the first few high-priority queues and thus be prioritized over long flows in general, which enables PIAS to emulate SJF without knowing flow sizes beforehand.We have implemented a PIAS prototype and evaluated PIAS through both testbed experiments and ns- 2 simulations. We show that PIAS is readily deployable with commodity switches and backward compatible with legacy TCP/IP stacks. Our evaluation results show that PIAS significantly outperforms existing information-agnostic schemes. For example, it reduces FCT by up to 50\% and 40\% over DCTCP [11] and L2DCT [27] respectively; and it only has a 4.9\% performance gap to an ideal information-aware scheme, pFabric [13], for short flows under a production DCN workload.},
booktitle = {Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation},
pages = {455–468},
numpages = {14},
location = {Oakland, CA},
series = {NSDI'15}
}

# Andy file name:
# 2015-grosvenor-et-al-queues-dont-matter-when-you-can-jump-them.pdf
@inproceedings{GSGWe2015,
author = {Grosvenor, Matthew P. and Schwarzkopf, Malte and Gog, Ionel and Watson, Robert N. M. and Moore, Andrew W. and Hand, Steven and Crowcroft, Jon},
title = {Queues don't matter when you can JUMP them!},
year = {2015},
isbn = {9781931971218},
publisher = {USENIX Association},
address = {USA},
abstract = {QJUMP is a simple and immediately deployable approach to controlling network interference in datacenter networks. Network interference occurs when congestion from throughput-intensive applications causes queueing that delays traffic from latency-sensitive applications. To mitigate network interference, QJUMP applies Internet QoS-inspired techniques to datacenter applications. Each application is assigned to a latency sensitivity level (or class). Packets from higher levels are rate-limited in the end host, but once allowed into the network can "jump-the-queue" over packets from lower levels. In settings with known node counts and link speeds, QJUMP can support service levels ranging from strictly bounded latency (but with low rate) through to line-rate throughput (but with high latency variance).We have implemented QJUMP as a Linux Traffic Control module. We show that QJUMP achieves bounded latency and reduces in-network interference by up to 300\texttimes{}, outperforming Ethernet Flow Control (802.3x), ECN (WRED) and DCTCP. We also show that QJUMP improves average flow completion times, performing close to or better than DCTCP and pFabric.},
booktitle = {Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation},
pages = {1–14},
numpages = {14},
location = {Oakland, CA},
series = {NSDI'15}
}

# alternate url
# https://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p465.pdf
# Andy file name:
# 2015-he-et-al-presto-edge-based-load-balancing-for-fast-datacenter-networks.pdf
@inproceedings{HRAFe2015,
author = {He, Keqiang and Rozner, Eric and Agarwal, Kanak and Felter, Wes and Carter, John and Akella, Aditya},
title = {Presto: Edge-based Load Balancing for Fast Datacenter Networks},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787507},
doi = {10.1145/2785956.2787507},
abstract = {Datacenter networks deal with a variety of workloads, ranging from latency-sensitive small flows to bandwidth-hungry large flows. Load balancing schemes based on flow hashing, e.g., ECMP, cause congestion when hash collisions occur and can perform poorly in asymmetric topologies. Recent proposals to load balance the network require centralized traffic engineering, multipath-aware transport, or expensive specialized hardware. We propose a mechanism that avoids these limitations by (i) pushing load-balancing functionality into the soft network edge (e.g., virtual switches) such that no changes are required in the transport layer, customer VMs, or networking hardware, and (ii) load balancing on fine-grained, near-uniform units of data (flowcells) that fit within end-host segment offload optimizations used to support fast networking speeds. We design and implement such a soft-edge load balancing scheme, called Presto, and evaluate it on a 10 Gbps physical testbed. We demonstrate the computational impact of packet reordering on receivers and propose a mechanism to handle reordering in the TCP receive offload functionality. Presto's performance closely tracks that of a single, non-blocking switch over many workloads and is adaptive to failures and topology asymmetry.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {465–478},
numpages = {14},
keywords = {load balancing, software-defined networking},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

# Andy file name:
# 2015-mittal-et-al-timely-rtt-based-congestion-control-for-the-datacenter.pdf
@inproceedings{MLDBe2015,
author = {Mittal, Radhika and Lam, Vinh The and Dukkipati, Nandita and Blem, Emily and Wassel, Hassan and Ghobadi, Monia and Vahdat, Amin and Wang, Yaogong and Wetherall, David and Zats, David},
title = {TIMELY: RTT-based Congestion Control for the Datacenter},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787510},
doi = {10.1145/2785956.2787510},
abstract = {Datacenter transports aim to deliver low latency messaging together with high throughput. We show that simple packet delay, measured as round-trip times at hosts, is an effective congestion signal without the need for switch feedback. First, we show that advances in NIC hardware have made RTT measurement possible with microsecond accuracy, and that these RTTs are sufficient to estimate switch queueing. Then we describe how TIMELY can adjust transmission rates using RTT gradients to keep packet latency low while delivering high bandwidth. We implement our design in host software running over NICs with OS-bypass capabilities. We show using experiments with up to hundreds of machines on a Clos network topology that it provides excellent performance: turning on TIMELY for OS-bypass messaging over a fabric with PFC lowers 99 percentile tail latency by 9X while maintaining near line-rate throughput. Our system also outperforms DCTCP running in an optimized kernel, reducing tail latency by $13$X. To the best of our knowledge, TIMELY is the first delay-based congestion control protocol for use in the datacenter, and it achieves its results despite having an order of magnitude fewer RTT signals (due to NIC offload) than earlier delay-based schemes such as Vegas.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {537–550},
numpages = {14},
keywords = {rdma, os-bypass, delay-based congestion control, datacenter transport},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

# Andy file name:
# 2015-dong-et-al-pcc-re-architecting-congestion-control-for-consistent-high-performance.pdf
@inproceedings{DLZGS2015,
author = {Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, P. Brighten and Schapira, Michael},
title = {PCC: re-architecting congestion control for consistent high performance},
year = {2015},
isbn = {9781931971218},
publisher = {USENIX Association},
address = {USA},
abstract = {TCP and its variants have suffered from surprisingly poor performance for decades. We argue the TCP family has little hope of achieving consistent high performance due to a fundamental architectural deficiency: hardwiring packet-level events to control responses. We propose Performance-oriented Congestion Control (PCC), a new congestion control architecture in which each sender continuously observes the connection between its actions and empirically experienced performance, enabling it to consistently adopt actions that result in high performance. We prove that PCC converges to a stable and fair equilibrium. Across many real-world and challenging environments, PCC shows consistent and often 10\texttimes{} performance improvement, with better fairness and stability than TCP. PCC requires no router hardware support or new packet format.},
booktitle = {Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation},
pages = {395–408},
numpages = {14},
location = {Oakland, CA},
series = {NSDI'15}
}

# alternate url
# https://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p523.pdf
# Andy file name:
# 2015-zhu-et-al-congestion-control-for-large-scale-rdma-deployments.pdf
@inproceedings{ZEFGe2015,
author = {Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming},
title = {Congestion Control for Large-Scale RDMA Deployments},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787484},
doi = {10.1145/2785956.2787484},
abstract = {Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (&lt; 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {523–536},
numpages = {14},
keywords = {datacenter transport, congestion control, RDMA, PFC, ECN},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

# Andy file name:
# 2015-lee-et-al-accurate-latency-based-congestion-feedback-for-datacenters.pdf
@inproceedings{LPJMH2015,
author = {Lee, Changhyun and Park, Chunjong and Jang, Keon and Moon, Sue and Han, Dongsu},
title = {Accurate latency-based congestion feedback for datacenters},
year = {2015},
isbn = {9781931971225},
publisher = {USENIX Association},
address = {USA},
abstract = {The nature of congestion feedback largely governs the behavior of congestion control. In datacenter networks, where RTTs are in hundreds of microseconds, accurate feedback is crucial to achieve both high utilization and low queueing delay. Proposals for datacenter congestion control predominantly leverage ECN or even explicit in-network feedback (e.g., RCP-type feedback) to minimize the queuing delay. In this work we explore latency-based feedback as an alternative and show its advantages over ECN. Against the common belief that such implicit feed-back is noisy and inaccurate, we demonstrate that latency-based implicit feedback is accurate enough to signal a single packet's queuing delay in 10 Gbps networks.DX enables accurate queuing delay measurements whose error falls within 1.98 and 0.53 microseconds using software-based and hardware-based latency measurements, respectively. This enables us to design a new congestion control algorithm that performs fine-grained control to adjust the congestion window just enough to achieve very low queuing delay while attaining full utilization. Our extensive evaluation shows that 1) the latency measurement accurately reflects the one-way queuing delay in single packet level; 2) the latency feedback can be used to perform practical and fine-grained congestion control in high-speed datacenter networks; and 3) DX outperforms DCTCP with 5.33\texttimes{} smaller median queueing delay at 1 Gbps and 1.57\texttimes{} at 10 Gbps.},
booktitle = {Proceedings of the 2015 USENIX Conference on Usenix Annual Technical Conference},
pages = {403–415},
numpages = {13},
location = {Santa Clara, CA},
series = {USENIX ATC '15}
}

# Andy file name:
# 2016-chen-et-al-scheduling-mix-flows-in-commodity-datacenters-with-karuna.pdf
@inproceedings{CCBA2016,
author = {Chen, Li and Chen, Kai and Bai, Wei and Alizadeh, Mohammad},
title = {Scheduling Mix-flows in Commodity Datacenters with Karuna},
year = {2016},
isbn = {9781450341936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934872.2934888},
doi = {10.1145/2934872.2934888},
abstract = {Cloud applications generate a mix of flows with and without deadlines. Scheduling such mix-flows is a key challenge; our experiments show that trivially combining existing schemes for deadline/non-deadline flows is problematic. For example, prioritizing deadline flows hurts flow completion time (FCT) for non-deadline flows, with minor improvement for deadline miss rate.We present Karuna, a first systematic solution for scheduling mix-flows. Our key insight is that deadline flows should meet their deadlines while minimally impacting the FCT of non-deadline flows. To achieve this goal, we design a novel Minimal-impact Congestion control Protocol (MCP) that handles deadline flows with as little bandwidth as possible. For non-deadline flows, we extend an existing FCT minimization scheme to schedule flows with known and unknown sizes. Karuna requires no switch modifications and is back- ward compatible with legacy TCP/IP stacks. Our testbed experiments and simulations show that Karuna effectively schedules mix-flows, for example, reducing the 95th percentile FCT of non-deadline flows by up to 47.78\% at high load compared to pFabric, while maintaining low (<5.8\%) deadline miss rate.},
booktitle = {Proceedings of the 2016 ACM SIGCOMM Conference},
pages = {174–187},
numpages = {14},
keywords = {Flow scheduling, Deadline, Datacenter networks},
location = {Florianopolis, Brazil},
series = {SIGCOMM '16}
}

# Andy file name:
# 2016-cardwell-et-al-bbr-congestion-based-congestion-control-measuring-bottleneck-bandwidth-and-round-trip-propagation-time.pdf
@article{CCGYJ2016,
author = {Cardwell, Neal and Cheng, Yuchung and Gunn, C. Stephen and Yeganeh, Soheil Hassas and Jacobson, Van},
title = {BBR: Congestion-Based Congestion Control: Measuring bottleneck bandwidth and round-trip propagation time},
year = {2016},
issue_date = {September-October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {1542-7730},
url = {https://doi.org/10.1145/3012426.3022184},
doi = {10.1145/3012426.3022184},
abstract = {When bottleneck buffers are large, loss-based congestion control keeps them full, causing bufferbloat. When bottleneck buffers are small, loss-based congestion control misinterprets loss as a signal of congestion, leading to low throughput. Fixing these problems requires an alternative to loss-based congestion control. Finding this alternative requires an understanding of where and how network congestion originates.},
journal = {Queue},
month = {oct},
pages = {20–53},
numpages = {34}
}

# Andy file name:
# 2017-handley-et-al-re-architecting-datacenter-networks-and-stacks-for-low-latency-and-high-performance.pdf
@inproceedings{HRAVe2017,
author = {Handley, Mark and Raiciu, Costin and Agache, Alexandru and Voinescu, Andrei and Moore, Andrew W. and Antichi, Gianni and W\'{o}jcik, Marcin},
title = {Re-architecting datacenter networks and stacks for low latency and high performance},
year = {2017},
isbn = {9781450346535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098822.3098825},
doi = {10.1145/3098822.3098825},
abstract = {Modern datacenter networks provide very high capacity via redundant Clos topologies and low switch latency, but transport protocols rarely deliver matching performance. We present NDP, a novel data-center transport architecture that achieves near-optimal completion times for short transfers and high flow throughput in a wide range of scenarios, including incast. NDP switch buffers are very shallow and when they fill the switches trim packets to headers and priority forward the headers. This gives receivers a full view of instantaneous demand from all senders, and is the basis for our novel, high-performance, multipath-aware transport protocol that can deal gracefully with massive incast events and prioritize traffic from different senders on RTT timescales. We implemented NDP in Linux hosts with DPDK, in a software switch, in a NetFPGA-based hardware switch, and in P4. We evaluate NDP's performance in our implementations and in large-scale simulations, simultaneously demonstrating support for very low-latency and high throughput.},
booktitle = {Proceedings of the Conference of the ACM Special Interest Group on Data Communication},
pages = {29–42},
numpages = {14},
keywords = {Datacenters, Network Stacks, Transport Protocols},
location = {Los Angeles, CA, USA},
series = {SIGCOMM '17}
}

# Andy file name:
# 2017-cho-et-al-credit-scheduled-delay-bounded-congestion-control-for-datacenters.pdf
@inproceedings{CJH2017,
author = {Cho, Inho and Jang, Keon and Han, Dongsu},
title = {Credit-Scheduled Delay-Bounded Congestion Control for Datacenters},
year = {2017},
isbn = {9781450346535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098822.3098840},
doi = {10.1145/3098822.3098840},
abstract = {Small RTTs (~tens of microseconds), bursty flow arrivals, and a large number of concurrent flows (thousands) in datacenters bring fundamental challenges to congestion control as they either force a flow to send at most one packet per RTT or induce a large queue build-up. The widespread use of shallow buffered switches also makes the problem more challenging with hosts generating many flows in bursts. In addition, as link speeds increase, algorithms that gradually probe for bandwidth take a long time to reach the fair-share. An ideal datacenter congestion control must provide 1) zero data loss, 2) fast convergence, 3) low buffer occupancy, and 4) high utilization. However, these requirements present conflicting goals.This paper presents a new radical approach, called ExpressPass, an end-to-end credit-scheduled, delay-bounded congestion control for datacenters. ExpressPass uses credit packets to control congestion even before sending data packets, which enables us to achieve bounded delay and fast convergence. It gracefully handles bursty flow arrivals. We implement ExpressPass using commodity switches and provide evaluations using testbed experiments and simulations. ExpressPass converges up to 80 times faster than DCTCP in 10 Gbps links, and the gap increases as link speeds become faster. It greatly improves performance under heavy incast workloads and significantly reduces the flow completion times, especially, for small and medium size flows compared to RCP, DCTCP, HULL, and DX under realistic workloads.},
booktitle = {Proceedings of the Conference of the ACM Special Interest Group on Data Communication},
pages = {239–252},
numpages = {14},
keywords = {Datacenter Network, Credit-based, Congestion Control},
location = {Los Angeles, CA, USA},
series = {SIGCOMM '17}
}

# Andy file name:
# 2017-arashloo-et-al-hotcocoa-hardware-congestion-control-abstractions.pdf
@inproceedings{AGRW2017,
author = {Arashloo, Mina Tahmasbi and Ghobadi, Monia and Rexford, Jennifer and Walker, David},
title = {HotCocoa: Hardware Congestion Control Abstractions},
year = {2017},
isbn = {9781450355698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152434.3152457},
doi = {10.1145/3152434.3152457},
abstract = {Congestion control in multi-tenant data centers is an active area of research because of its significant impact on customer experience, and, consequently, on revenue. Therefore, new algorithms and protocols are expected to emerge as the Cloud evolves. Deploying new congestion control algorithms in the end host's hypervisor allows frequent updates, but processing packets at high rates in the hypervisor and implementing the elements of a congestion control algorithm, such as traffic shapers and timestamps, in software have well-studied inaccuracies and CPU inefficiencies. In this paper, we argue for implementing the entire congestion control algorithm in programmable NICs. To do so, we identify the absence of hardware-aware programming abstractions as the most immediate challenge and solve it using a simple high-level domain specific language called HotCocoa. HotCocoa lies at a sweet spot between the ability to express a broad set of congestion control algorithms and efficient hardware implementation. It offers a set of hardware-aware COngestion COntrol Abstractions that enable operators to specify their algorithm without having to worry about low-level hardware primitives. To evaluate HotCocoa, we implement four congestion control algorithms (Reno, DCTCP, PCC, and TIMELY) and use simulations to show that HotCocoa's implementation of Reno perfectly tracks the behavior of a native implementation in C++.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
pages = {108–114},
numpages = {7},
location = {<conf-loc>, <city>Palo Alto</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {HotNets '17}
}

# Andy file name:
# 2017-goyal-et-al-rethinking-congestion-control-for-cellular-networks.pdf
@inproceedings{GAB2017,
author = {Goyal, Prateesh and Alizadeh, Mohammad and Balakrishnan, Hari},
title = {Rethinking Congestion Control for Cellular Networks},
year = {2017},
isbn = {9781450355698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152434.3152437},
doi = {10.1145/3152434.3152437},
abstract = {We propose Accel-Brake Control (ABC), a protocol that integrates a simple and deployable signaling scheme at cellular base stations with an endpoint mechanism to respond to these signals. The key idea is for the base station to enable each sender to achieve a computed target rate by marking each packet with an "accelerate" or "brake" notification, which causes the sender to either slightly increase or slightly reduce its congestion window. ABC is designed to rapidly acquire any capacity that opens up, a common occurrence in cellular networks, while responding promptly to congestion. It is also incrementally deployable using existing ECN infrastructure and can co-exist with legacy ECN routers. Preliminary results obtained over cellular network traces show that ABC outperforms prior approaches significantly.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
pages = {29–35},
numpages = {7},
location = {<conf-loc>, <city>Palo Alto</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {HotNets '17}
}

# Andy file name:
# 2018-montazeri-et-al-homa-a-receiver-driven-low-latency-transport-protocol-using-network-priorities.pdf
@inproceedings{MLAO2018,
author = {Montazeri, Behnam and Li, Yilong and Alizadeh, Mohammad and Ousterhout, John},
title = {Homa: a receiver-driven low-latency transport protocol using network priorities},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230564},
doi = {10.1145/3230543.3230564},
abstract = {Homa is a new transport protocol for datacenter networks. It provides exceptionally low latency, especially for workloads with a high volume of very short messages, and it also supports large messages and high network utilization. Homa uses in-network priority queues to ensure low latency for short messages; priority allocation is managed dynamically by each receiver and integrated with a receiver-driven flow control mechanism. Homa also uses controlled overcommitment of receiver downlinks to ensure efficient bandwidth utilization at high load. Our implementation of Homa delivers 99th percentile round-trip times less than 15 μs for short messages on a 10 Gbps network running at 80\% load. These latencies are almost 100x lower than the best published measurements of an implementation. In simulations, Homa's latency is roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for almost all message sizes and workloads. Homa can also sustain higher network loads than pFabric, pHost, or PIAS.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {221–235},
numpages = {15},
keywords = {data centers, low latency, network stacks, transport protocols},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

# Andy file name:
# 2018-narayan-et-al-restructuring-endpoint-congestion-control.pdf
@inproceedings{NCRGe2018,
author = {Narayan, Akshay and Cangialosi, Frank and Raghavan, Deepti and Goyal, Prateesh and Narayana, Srinivas and Mittal, Radhika and Alizadeh, Mohammad and Balakrishnan, Hari},
title = {Restructuring endpoint congestion control},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230553},
doi = {10.1145/3230543.3230553},
abstract = {This paper describes the implementation and evaluation of a system to implement complex congestion control functions by placing them in a separate agent outside the datapath. Each datapath---such as the Linux kernel TCP, UDP-based QUIC, or kernel-bypass transports like mTCP-on-DPDK---summarizes information about packet round-trip times, receptions, losses, and ECN via a well-defined interface to algorithms running in the off-datapath Congestion Control Plane (CCP). The algorithms use this information to control the datapath's congestion window or pacing rate. Algorithms written in CCP can run on multiple datapaths. CCP improves both the pace of development and ease of maintenance of congestion control algorithms by providing better, modular abstractions, and supports aggregation capabilities of the Congestion Manager, all with one-time changes to datapaths. CCP also enables new capabilities, such as Copa in Linux TCP, several algorithms running on QUIC and mTCP/DPDK, and the use of signal processing algorithms to detect whether cross-traffic is ACK-clocked. Experiments with our user-level Linux CCP implementation show that CCP algorithms behave similarly to kernel algorithms, and incur modest CPU overhead of a few percent.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {30–43},
numpages = {14},
keywords = {operating systems, congestion control},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

# alternate url
# https://dspace.mit.edu/bitstream/handle/1721.1/137381/1802.08730.pdf
# Andy file name:
# 2018-goyal-et-al-elasticity-detection-a-building-block-for-delay-sensitive-congestion-control.pdf
@inproceedings{GNCRe2018,
author = {Goyal, Prateesh and Narayan, Akshay and Cangialosi, Frank and Raghavan, Deepti and Narayana, Srinivas and Alizadeh, Mohammad and Balakrishnan, Hari},
title = {Elasticity Detection: A Building Block for Delay-Sensitive Congestion Control},
year = {2018},
isbn = {9781450355858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3232755.3232772},
doi = {10.1145/3232755.3232772},
abstract = {This paper develops a technique to detect whether the cross traffic competing with a flow is elastic or not, and shows how to use the elasticity detector to improve congestion control. If the cross traffic is elastic, i.e., made up of buffer-filling flows like Cubic or Reno, then one should use a scheme that competes well with such traffic. Such a scheme will not be able to control delays because the cross traffic will not cooperate. If, however, cross traffic is inelastic, then one can use a suitable delay-sensitive congestion control algorithm, which can control delays, but which would have obtained dismal throughput when run concurrently with a buffer-filling algorithm.We use the elasticity detector to demonstrate a congestion control framework that always achieves high utilization, but which can also achieve low delays when cross traffic permits it. The technique uses an asymmetric sinusoidal pulse patternand estimates elasticity by computing the frequency response(FFT) of the cross traffic estimate; we have measured its accuracy to be over 90\%. We have developed Nimbus, a protocol that explicitly switches between TCP-competitive and delay-sensitive modes using the elasticity detector. Our results on emulated and real-world paths show that Nimbus achieves throughput comparable to or better than Cubic always, but with delays that are much lower when cross traffic is inelastic. Unlike BBR, Nimbus is fair to Cubic, and has significantly lower delay in all cases; for example, on real-world paths, Nimbus has 11\% lower throughput but at 40-50 ms lower packet delay.},
booktitle = {Proceedings of the Applied Networking Research Workshop},
pages = {75},
numpages = {1},
location = {Montreal, QC, Canada},
series = {ANRW '18}
}

# alternate url
# https://www.usenix.org/system/files/conference/atc18/atc18-yan-francis.pdf
# Andy file name:
# 2018-yan-et-al-pantheon-the-training-ground-for-internet-congestion-control-research.pdf
@inproceedings{YMHRe2018,
author = {Yan, Francis Y. and Ma, Jestin and Hill, Greg D. and Raghavan, Deepti and Wahby, Riad S. and Levis, Philip and Winstein, Keith},
title = {Pantheon: the training ground for internet congestion-control research},
year = {2018},
isbn = {9781931971447},
publisher = {USENIX Association},
address = {USA},
abstract = {Internet transport algorithms are foundational to the performance of network applications. But a number of practical challenges make it difficult to evaluate new ideas and algorithms in a reproducible manner. We present the Pantheon, a system that addresses this by serving as a community "training ground" for research on Internet transport protocols and congestion control (https://pantheon.stanford.edu). It allows network researchers to benefit from and contribute to a common set of benchmark algorithms, a shared evaluation platform, and a public archive of results.We present three results showing the Pantheon's value as a research tool. First, we describe a measurement study from more than a year of data, indicating that congestion-control schemes vary dramatically in their relative performance as a function of path dynamics. Second, the Pantheon generates calibrated network emulators that capture the diverse performance of real Internet paths. These enable reproducible and rapid experiments that closely approximate real-world results. Finally, we describe the Pantheon's contribution to developing new congestion-control schemes, two of which were published at USENIX NSDI 2018, as well as data-driven neural-network-based congestion-control schemes that can be trained to achieve good performance over the real Internet.},
booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
pages = {731–743},
numpages = {13},
location = {Boston, MA, USA},
series = {USENIX ATC '18}
}

# alternate url
# https://www.usenix.org/system/files/conference/nsdi18/nsdi18-arun.pdf
# Andy file name:
# 2018-arun-et-al-copa-practical-delay-based-congestion-control-for-the-internet.pdf
@inproceedings {AB2018,
	author = {Venkat Arun and Hari Balakrishnan},
	title = {Copa: Congestion Control Combining Objective Optimization with Window Adjustments},
	booktitle = {15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18)},
	year = {2018},
	address = {Renton, WA},
	url = {https://www.usenix.org/conference/nsdi18/presentation/arun},
	publisher = {{USENIX} Association},
	month = apr
}

# Andy file name:
# 2021-arun-et-al-toward-formally-verifying-congestion-control-behavior.pdf
@inproceedings{AASAB2021,
author = {Arun, Venkat and Arashloo, Mina Tahmasbi and Saeed, Ahmed and Alizadeh, Mohammad and Balakrishnan, Hari},
title = {Toward formally verifying congestion control behavior},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472912},
doi = {10.1145/3452296.3472912},
abstract = {The diversity of paths on the Internet makes it difficult for designers and operators to confidently deploy new congestion control algorithms (CCAs) without extensive real-world experiments, but such capabilities are not available to most of the networking community. And even when they are available, understanding why a CCA underperforms by trawling through massive amounts of statistical data from network connections is challenging. The history of congestion control is replete with many examples of surprising and unanticipated behaviors unseen in simulation but observed on real-world paths. In this paper, we propose initial steps toward modeling and improving our confidence in a CCA's behavior. We have developed CCAC, a tool that uses formal verification to establish certain properties of CCAs. It is able to prove hypotheses about CCAs or generate counterexamples for invalid hypotheses. With CCAC, a designer can not only gain greater confidence prior to deployment to avoid unpleasant surprises, but can also use the counterexamples to iteratively improvetheir algorithm. We have modeled additive-increase/multiplicative-decrease (AIMD), Copa, and BBR with CCAC, and describe some surprising results from the exercise.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {1–16},
numpages = {16},
keywords = {formal verification, congestion control, WAN transport},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

# Andy file name:
# 2013-dean-et-al-the-tail-at-scale.pdf
@article{DB2013,
author = {Dean, Jeffrey and Barroso, Luiz Andr\'{e}},
title = {The tail at scale},
year = {2013},
issue_date = {February 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/2408776.2408794},
doi = {10.1145/2408776.2408794},
abstract = {Software techniques that tolerate latency variability are vital to building responsive large-scale Web services.},
journal = {Commun. ACM},
month = {feb},
pages = {74–80},
numpages = {7}
}
